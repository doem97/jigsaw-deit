6845 (0.6845)  jigsaw_acc: 0.5703 (0.5804)  time: 0.3760  data: 0.1264  max mem: 1809
Epoch: [49]  [ 560/1251]  eta: 0:04:31  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5805)  time: 0.3603  data: 0.1147  max mem: 1809
Epoch: [49]  [ 570/1251]  eta: 0:04:27  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5803)  time: 0.3808  data: 0.1266  max mem: 1809
Epoch: [49]  [ 580/1251]  eta: 0:04:23  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5802)  time: 0.3699  data: 0.1344  max mem: 1809
Epoch: [49]  [ 590/1251]  eta: 0:04:19  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5801)  time: 0.3891  data: 0.1543  max mem: 1809
Epoch: [49]  [ 600/1251]  eta: 0:04:15  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5625 (0.5798)  time: 0.3965  data: 0.1562  max mem: 1809
Epoch: [49]  [ 610/1251]  eta: 0:04:11  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5625 (0.5796)  time: 0.3828  data: 0.1444  max mem: 1809
Epoch: [49]  [ 620/1251]  eta: 0:04:07  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5781 (0.5796)  time: 0.3824  data: 0.1392  max mem: 1809
Epoch: [49]  [ 630/1251]  eta: 0:04:03  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5625 (0.5794)  time: 0.3789  data: 0.1322  max mem: 1809
Epoch: [49]  [ 640/1251]  eta: 0:03:59  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5794)  time: 0.3855  data: 0.1378  max mem: 1809
Epoch: [49]  [ 650/1251]  eta: 0:03:55  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5793)  time: 0.3857  data: 0.1343  max mem: 1809
Epoch: [49]  [ 660/1251]  eta: 0:03:51  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5794)  time: 0.3846  data: 0.1281  max mem: 1809
Epoch: [49]  [ 670/1251]  eta: 0:03:47  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5547 (0.5789)  time: 0.3858  data: 0.1289  max mem: 1809
Epoch: [49]  [ 680/1251]  eta: 0:03:43  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5781 (0.5790)  time: 0.3807  data: 0.1162  max mem: 1809
Epoch: [49]  [ 690/1251]  eta: 0:03:39  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5859 (0.5788)  time: 0.3783  data: 0.1181  max mem: 1809
Epoch: [49]  [ 700/1251]  eta: 0:03:35  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5703 (0.5786)  time: 0.3891  data: 0.1340  max mem: 1809
Epoch: [49]  [ 710/1251]  eta: 0:03:31  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5703 (0.5789)  time: 0.3807  data: 0.1349  max mem: 1809
Epoch: [49]  [ 720/1251]  eta: 0:03:27  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5703 (0.5787)  time: 0.3887  data: 0.1425  max mem: 1809
Epoch: [49]  [ 730/1251]  eta: 0:03:23  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5790)  time: 0.3841  data: 0.1364  max mem: 1809
Epoch: [49]  [ 740/1251]  eta: 0:03:19  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.6094 (0.5792)  time: 0.3720  data: 0.1205  max mem: 1809
Epoch: [49]  [ 750/1251]  eta: 0:03:15  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5793)  time: 0.3933  data: 0.1345  max mem: 1809
Epoch: [49]  [ 760/1251]  eta: 0:03:11  lr: 0.000004  loss_total: 0.6843 (0.6845)  loss_jigsaw: 0.6843 (0.6845
)  jigsaw_acc: 0.5781 (0.5793)  time: 0.3813  data: 0.1328  max mem: 1809
Epoch: [49]  [ 770/1251]  eta: 0:03:07  lr: 0.000004  loss_total: 0.6843 (0.6845)  loss_jigsaw: 0.6843 (0.6845
)  jigsaw_acc: 0.5703 (0.5792)  time: 0.3782  data: 0.1330  max mem: 1809
Epoch: [49]  [ 780/1251]  eta: 0:03:04  lr: 0.000004  loss_total: 0.6843 (0.6845)  loss_jigsaw: 0.6843 (0.6845
)  jigsaw_acc: 0.5703 (0.5791)  time: 0.4163  data: 0.1375  max mem: 1809
Epoch: [49]  [ 790/1251]  eta: 0:02:59  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5790)  time: 0.3778  data: 0.0983  max mem: 1809
Epoch: [49]  [ 800/1251]  eta: 0:02:56  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5703 (0.5787)  time: 0.3628  data: 0.1080  max mem: 1809
Epoch: [49]  [ 810/1251]  eta: 0:02:52  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5703 (0.5787)  time: 0.4018  data: 0.1336  max mem: 1809
Epoch: [49]  [ 820/1251]  eta: 0:02:48  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5859 (0.5788)  time: 0.4005  data: 0.1406  max mem: 1809
Epoch: [49]  [ 830/1251]  eta: 0:02:44  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5789)  time: 0.3990  data: 0.1247  max mem: 1809
Epoch: [49]  [ 840/1251]  eta: 0:02:40  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5781 (0.5790)  time: 0.3877  data: 0.0969  max mem: 1809
Epoch: [49]  [ 850/1251]  eta: 0:02:36  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5786)  time: 0.3681  data: 0.0983  max mem: 1809
Epoch: [49]  [ 860/1251]  eta: 0:02:32  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5625 (0.5785)  time: 0.3776  data: 0.1317  max mem: 1809
Epoch: [49]  [ 870/1251]  eta: 0:02:28  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5547 (0.5784)  time: 0.3948  data: 0.1463  max mem: 1809
Epoch: [49]  [ 880/1251]  eta: 0:02:24  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5787)  time: 0.3796  data: 0.1232  max mem: 1809
Epoch: [49]  [ 890/1251]  eta: 0:02:20  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5781 (0.5785)  time: 0.3775  data: 0.1282  max mem: 1809
Epoch: [49]  [ 900/1251]  eta: 0:02:16  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5547 (0.5782)  time: 0.3858  data: 0.1399  max mem: 1809
Epoch: [49]  [ 910/1251]  eta: 0:02:12  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5784)  time: 0.3793  data: 0.1461  max mem: 1809
Epoch: [49]  [ 920/1251]  eta: 0:02:09  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5938 (0.5785)  time: 0.3797  data: 0.1332  max mem: 1809
Epoch: [49]  [ 930/1251]  eta: 0:02:05  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5938 (0.5785)  time: 0.4014  data: 0.1215  max mem: 1809
Epoch: [49]  [ 940/1251]  eta: 0:02:01  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5938 (0.5786)  time: 0.3827  data: 0.1138  max mem: 1809
Epoch: [49]  [ 950/1251]  eta: 0:01:57  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5787)  time: 0.3560  data: 0.1174  max mem: 1809
Epoch: [49]  [ 960/1251]  eta: 0:01:53  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5788)  time: 0.3785  data: 0.1294  max mem: 1809
Epoch: [49]  [ 970/1251]  eta: 0:01:49  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5781 (0.5786)  time: 0.3905  data: 0.1362  max mem: 1809
Epoch: [49]  [ 980/1251]  eta: 0:01:45  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5547 (0.5785)  time: 0.3990  data: 0.1317  max mem: 1809
Epoch: [49]  [ 990/1251]  eta: 0:01:41  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5786)  time: 0.3976  data: 0.1102  max mem: 1809
Epoch: [49]  [1000/1251]  eta: 0:01:37  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5859 (0.5786)  time: 0.3457  data: 0.0943  max mem: 1809
Epoch: [49]  [1010/1251]  eta: 0:01:33  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5938 (0.5785)  time: 0.3665  data: 0.1159  max mem: 1809
Epoch: [49]  [1020/1251]  eta: 0:01:29  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5859 (0.5786)  time: 0.3919  data: 0.1343  max mem: 1809
Epoch: [49]  [1030/1251]  eta: 0:01:25  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5859 (0.5785)  time: 0.3725  data: 0.1327  max mem: 1809
Epoch: [49]  [1040/1251]  eta: 0:01:22  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5625 (0.5783)  time: 0.3828  data: 0.1306  max mem: 1809
Epoch: [49]  [1050/1251]  eta: 0:01:18  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5625 (0.5783)  time: 0.3879  data: 0.1340  max mem: 1809
Epoch: [49]  [1060/1251]  eta: 0:01:14  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5625 (0.5783)  time: 0.3865  data: 0.1364  max mem: 1809
Epoch: [49]  [1070/1251]  eta: 0:01:10  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5781)  time: 0.3809  data: 0.1299  max mem: 1809
Epoch: [49]  [1080/1251]  eta: 0:01:06  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5547 (0.5781)  time: 0.3870  data: 0.1402  max mem: 1809
Epoch: [49]  [1090/1251]  eta: 0:01:02  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5779)  time: 0.3923  data: 0.1420  max mem: 1809
Epoch: [49]  [1100/1251]  eta: 0:00:58  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5779)  time: 0.3906  data: 0.1485  max mem: 1809
Epoch: [49]  [1110/1251]  eta: 0:00:54  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5703 (0.5779)  time: 0.3986  data: 0.1405  max mem: 1809
Epoch: [49]  [1120/1251]  eta: 0:00:50  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5781 (0.5779)  time: 0.3618  data: 0.0992  max mem: 1809
Epoch: [49]  [1130/1251]  eta: 0:00:46  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5781 (0.5780)  time: 0.3534  data: 0.1169  max mem: 1809
Epoch: [49]  [1140/1251]  eta: 0:00:43  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5547 (0.5778)  time: 0.3855  data: 0.1492  max mem: 1809
Epoch: [49]  [1150/1251]  eta: 0:00:39  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5547 (0.5779)  time: 0.3796  data: 0.1566  max mem: 1809
Epoch: [49]  [1160/1251]  eta: 0:00:35  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5778)  time: 0.3799  data: 0.1540  max mem: 1809
Epoch: [49]  [1170/1251]  eta: 0:00:31  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5781 (0.5780)  time: 0.3851  data: 0.1397  max mem: 1809
Epoch: [49]  [1180/1251]  eta: 0:00:27  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.6016 (0.5781)  time: 0.3893  data: 0.1391  max mem: 1809
Epoch: [49]  [1190/1251]  eta: 0:00:23  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5703 (0.5780)  time: 0.3807  data: 0.1334  max mem: 1809
Epoch: [49]  [1200/1251]  eta: 0:00:19  lr: 0.000004  loss_total: 0.6846 (0.6845)  loss_jigsaw: 0.6846 (0.6845
)  jigsaw_acc: 0.5703 (0.5779)  time: 0.3780  data: 0.1457  max mem: 1809
Epoch: [49]  [1210/1251]  eta: 0:00:15  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5703 (0.5779)  time: 0.3832  data: 0.1492  max mem: 1809
Epoch: [49]  [1220/1251]  eta: 0:00:12  lr: 0.000004  loss_total: 0.6845 (0.6845)  loss_jigsaw: 0.6845 (0.6845
)  jigsaw_acc: 0.5625 (0.5777)  time: 0.3769  data: 0.1335  max mem: 1809
Epoch: [49]  [1230/1251]  eta: 0:00:08  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5779)  time: 0.3737  data: 0.1392  max mem: 1809
Epoch: [49]  [1240/1251]  eta: 0:00:04  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5780)  time: 0.3721  data: 0.1421  max mem: 1809
Epoch: [49]  [1250/1251]  eta: 0:00:00  lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845
)  jigsaw_acc: 0.5703 (0.5780)  time: 0.2714  data: 0.0960  max mem: 1809
Epoch: [49] Total time: 0:08:03 (0.3867 s / it)
Averaged stats: lr: 0.000004  loss_total: 0.6844 (0.6845)  loss_jigsaw: 0.6844 (0.6845)  jigsaw_acc: 0.5703 (0
.5777)
Test:  [  0/391]  eta: 0:32:32  loss: 2.6609 (2.6609)  acc: 0.9297 (0.9297)  time: 4.9927  data: 4.8641  max m
em: 1809
Test:  [ 10/391]  eta: 0:05:03  loss: 2.6394 (2.6423)  acc: 0.9609 (0.9524)  time: 0.7963  data: 0.7398  max m
em: 1809
Test:  [ 20/391]  eta: 0:03:34  loss: 2.6394 (2.6419)  acc: 0.9453 (0.9494)  time: 0.3578  data: 0.3001  max m
em: 1809
Test:  [ 30/391]  eta: 0:03:12  loss: 2.6390 (2.6407)  acc: 0.9531 (0.9514)  time: 0.3869  data: 0.3294  max m
em: 1809
Test:  [ 40/391]  eta: 0:02:56  loss: 2.6356 (2.6399)  acc: 0.9531 (0.9537)  time: 0.4216  data: 0.3663  max m
em: 1809
Test:  [ 50/391]  eta: 0:02:43  loss: 2.6375 (2.6401)  acc: 0.9531 (0.9531)  time: 0.3983  data: 0.3447  max m
em: 1809
Test:  [ 60/391]  eta: 0:02:36  loss: 2.6423 (2.6408)  acc: 0.9453 (0.9512)  time: 0.4172  data: 0.3554  max m
em: 1809
Test:  [ 70/391]  eta: 0:02:31  loss: 2.6412 (2.6407)  acc: 0.9453 (0.9513)  time: 0.4519  data: 0.3869  max m
em: 1809
Test:  [ 80/391]  eta: 0:02:23  loss: 2.6382 (2.6407)  acc: 0.9453 (0.9502)  time: 0.4254  data: 0.3749  max m
em: 1809
Test:  [ 90/391]  eta: 0:02:19  loss: 2.6394 (2.6406)  acc: 0.9609 (0.9514)  time: 0.4327  data: 0.3782  max m
em: 1809
Test:  [100/391]  eta: 0:02:14  loss: 2.6394 (2.6404)  acc: 0.9688 (0.9519)  time: 0.4681  data: 0.4143  max m
em: 1809
Test:  [110/391]  eta: 0:02:09  loss: 2.6370 (2.6402)  acc: 0.9609 (0.9525)  time: 0.4469  data: 0.3980  max m
em: 1809
Test:  [120/391]  eta: 0:02:04  loss: 2.6370 (2.6400)  acc: 0.9609 (0.9533)  time: 0.4410  data: 0.3890  max m
em: 1809
Test:  [130/391]  eta: 0:01:59  loss: 2.6383 (2.6401)  acc: 0.9609 (0.9531)  time: 0.4443  data: 0.3889  max m
em: 1809
Test:  [140/391]  eta: 0:01:53  loss: 2.6390 (2.6401)  acc: 0.9531 (0.9526)  time: 0.4163  data: 0.3656  max m
em: 1809
Test:  [150/391]  eta: 0:01:48  loss: 2.6390 (2.6400)  acc: 0.9531 (0.9528)  time: 0.4131  data: 0.3701  max m
em: 1809
Test:  [160/391]  eta: 0:01:43  loss: 2.6394 (2.6404)  acc: 0.9453 (0.9520)  time: 0.4140  data: 0.3766  max m
em: 1809
Test:  [170/391]  eta: 0:01:38  loss: 2.6412 (2.6404)  acc: 0.9375 (0.9516)  time: 0.3929  data: 0.3612  max m
em: 1809
Test:  [180/391]  eta: 0:01:34  loss: 2.6385 (2.6404)  acc: 0.9453 (0.9513)  time: 0.4302  data: 0.3937  max m
em: 1809
Test:  [190/391]  eta: 0:01:29  loss: 2.6392 (2.6404)  acc: 0.9531 (0.9517)  time: 0.4554  data: 0.4168  max m
em: 1809
Test:  [200/391]  eta: 0:01:24  loss: 2.6384 (2.6403)  acc: 0.9531 (0.9516)  time: 0.4034  data: 0.3681  max m
em: 1809
Test:  [210/391]  eta: 0:01:19  loss: 2.6376 (2.6402)  acc: 0.9531 (0.9516)  time: 0.3686  data: 0.3315  max m
em: 1809
Test:  [220/391]  eta: 0:01:15  loss: 2.6376 (2.6402)  acc: 0.9531 (0.9516)  time: 0.4021  data: 0.3615  max m
em: 1809
Test:  [230/391]  eta: 0:01:09  loss: 2.6368 (2.6401)  acc: 0.9609 (0.9520)  time: 0.3894  data: 0.3495  max m
em: 1809
Test:  [240/391]  eta: 0:01:05  loss: 2.6361 (2.6400)  acc: 0.9531 (0.9521)  time: 0.3656  data: 0.3285  max m
em: 1809
Test:  [250/391]  eta: 0:01:00  loss: 2.6359 (2.6398)  acc: 0.9531 (0.9523)  time: 0.3900  data: 0.3532  max m
em: 1809
Test:  [260/391]  eta: 0:00:56  loss: 2.6383 (2.6399)  acc: 0.9453 (0.9520)  time: 0.3862  data: 0.3511  max m
em: 1809
Test:  [270/391]  eta: 0:00:51  loss: 2.6388 (2.6399)  acc: 0.9453 (0.9519)  time: 0.4054  data: 0.3702  max m
em: 1809
Test:  [280/391]  eta: 0:00:47  loss: 2.6366 (2.6398)  acc: 0.9609 (0.9520)  time: 0.3969  data: 0.3608  max m
em: 1809
Test:  [290/391]  eta: 0:00:43  loss: 2.6370 (2.6398)  acc: 0.9609 (0.9525)  time: 0.3931  data: 0.3564  max m
em: 1809
Test:  [300/391]  eta: 0:00:38  loss: 2.6394 (2.6398)  acc: 0.9609 (0.9524)  time: 0.4394  data: 0.4012  max m
em: 1809
Test:  [310/391]  eta: 0:00:34  loss: 2.6386 (2.6398)  acc: 0.9531 (0.9525)  time: 0.4072  data: 0.3704  max m
em: 1809
Test:  [320/391]  eta: 0:00:30  loss: 2.6346 (2.6397)  acc: 0.9531 (0.9525)  time: 0.3535  data: 0.3191  max m
em: 1809
Test:  [330/391]  eta: 0:00:25  loss: 2.6355 (2.6396)  acc: 0.9531 (0.9524)  time: 0.3653  data: 0.3258  max m
em: 1809
Test:  [340/391]  eta: 0:00:21  loss: 2.6379 (2.6397)  acc: 0.9453 (0.9523)  time: 0.3816  data: 0.3431  max m
em: 1809
Test:  [350/391]  eta: 0:00:17  loss: 2.6380 (2.6396)  acc: 0.9531 (0.9524)  time: 0.3970  data: 0.3615  max m
em: 1809
Test:  [360/391]  eta: 0:00:13  loss: 2.6380 (2.6396)  acc: 0.9531 (0.9522)  time: 0.4043  data: 0.3683  max m
em: 1809
Test:  [370/391]  eta: 0:00:08  loss: 2.6414 (2.6397)  acc: 0.9453 (0.9521)  time: 0.4000  data: 0.3659  max m
em: 1809
Test:  [380/391]  eta: 0:00:04  loss: 2.6381 (2.6397)  acc: 0.9453 (0.9521)  time: 0.3709  data: 0.3364  max m
em: 1809
Test:  [390/391]  eta: 0:00:00  loss: 2.6371 (2.6396)  acc: 0.9531 (0.9521)  time: 0.2490  data: 0.2176  max m
em: 1809
Test: Total time: 0:02:40 (0.4116 s / it)
* Acc 0.952 loss 2.640
Max accuracy: 0.95%
Training time 8:55:02
wandb: Waiting for W&B process to finish... (success).
wandb: - 1.426 MB of 1.426 MB uploaded (0.000 MB deduped)
wandb: Run history:
wandb:  jigsaw_acc ▁▄▅▄▅▆▆▆▇▆▆▆▇▆▅▇▇▇▇▇▇▆▇▆▇▆██▇▇▇▇▇▆▇▇▇▇██
wandb: jigsaw_loss █▂▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  total_loss █▂▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:
wandb: Run summary:
wandb:  jigsaw_acc 0.57031
wandb: jigsaw_loss 0.68442
wandb:  total_loss 0.68442
wandb:
wandb: 🚀 View run in1k_jigsaw_small_patch56_336_e30_c1000 at: https://wandb.ai/doem97/Puzzle/runs/xey3sucx
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20231031_191806-xey3sucx/logs
(study) root@e20a9f9e5a00:/workspace/study/permvit# la
.git         augment.py   engine_jigsaw.py   losses.py       models_jigsaw.py  perm6x6x50.npy    preds
     tox.ini
.gitignore   data         exps               main.py         outputs           perm6x6x500.npy   requirements.
txt  utils.py
.vscode      datasets.py  inference_perm.py  main_jigsaw.py  perm6x6x1000.npy  perm6x6x50x6.npy  samplers.py
     wandb
__pycache__  engine.py    log.txt            models.py       perm6x6x3.npy     permsets.py       scripts
(study) root@e20a9f9e5a00:/workspace/study/permvit# bash ./exps/dgx/jigsaw_small_p56_336_in1k_c1000frcl50.sh
/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module
torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See
https://pytorch.org/docs/stable/distributed.html#launch-utility for
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being o
verloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
| distributed init (rank 2): env://
| distributed init (rank 1): env://
| distributed init (rank 6): env://
| distributed init (rank 3): env://
| distributed init (rank 4): env://
| distributed init (rank 7): env://
| distributed init (rank 5): env://
| distributed init (rank 0): env://
wandb: Currently logged in as: doem97. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 974, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 719, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 974, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 719, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 974, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 719, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 974, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 719, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 974, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 719, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 974, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 719, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 974, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 719, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /workspace/study/permvit/wandb/run-20231102_185053-yucviah5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k_jigsaw_small_patch56_336_e30_c1000frcl50
wandb: ⭐️ View project at https://wandb.ai/doem97/Puzzle
wandb: 🚀 View run at https://wandb.ai/doem97/Puzzle/runs/yucviah5
Namespace(batch_size=128, epochs=50, bce_loss=True, unscale_lr=True, rec=False, freeze=True, model='jigsaw_sma
ll_patch56_336', input_size=336, permcls=1000, drop=0.0, drop_path=0.1, model_ema=True, model_ema_decay=0.9999
6, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight
_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, mi
n_lr=1e-08, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_ji
tter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=None, train_interpolation='bicubic', repeated_aug=True, train_m
ode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cut
mix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety
_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='./ou
tputs/in1k_jigsaw_small_patch56_336_e30_c1000/best_checkpoint.pth', attn_only=False, data_path='/workspace/dat
a/imagenet/ILSVRC/Data/CLS-LOC', data_set='IMNET', nb_classes=1000, inat_category='name', output_dir='./output
s/in1k_jigsaw_small_patch56_336_e30_c1000frcl50', device='cuda', seed=0, resume='', start_epoch=0, eval=False,
 eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=8, dist_url='env://', local_
rank=0, use_jigsaw=True, use_cls=True, lambda_rec=0.1, mask_ratio=0.0, rank=0, gpu=0, distributed=True, dist_b
ackend='nccl')
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 20138 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 20139) of binary:
 /opt/conda/envs/study/bin/python
Traceback (most recent call last):
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __
call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in la
unch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
main_jigsaw.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-11-02_18:51:05
  host      : e20a9f9e5a00
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 20140)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-11-02_18:51:05
  host      : e20a9f9e5a00
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 20141)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-11-02_18:51:05
  host      : e20a9f9e5a00
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 20142)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-11-02_18:51:05
  host      : e20a9f9e5a00
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 20143)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-11-02_18:51:05
  host      : e20a9f9e5a00
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 20144)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-11-02_18:51:05
  host      : e20a9f9e5a00
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 20145)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-02_18:51:05
  host      : e20a9f9e5a00
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 20139)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(study) root@e20a9f9e5a00:/workspace/study/permvit# bash ./exps/dgx/jigsaw_small_p56_336_in1k_c1000frcl50.sh
/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module
torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See
https://pytorch.org/docs/stable/distributed.html#launch-utility for
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being o
verloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
| distributed init (rank 2): env://
| distributed init (rank 4): env://
| distributed init (rank 6): env://
| distributed init (rank 1): env://
| distributed init (rank 0): env://
| distributed init (rank 3): env://
| distributed init (rank 7): env://
| distributed init (rank 5): env://
wandb: Currently logged in as: doem97. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 976, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 721, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 976, in <module>
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 721, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 976, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 721, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 976, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 721, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 976, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 721, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 976, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 721, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 976, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 721, in main
    model.load_state_dict(checkpoint_model, strict=False)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1671, in load_sta
te_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for JigsawVisionTransformer:
        size mismatch for cls_head.0.weight: copying a param with shape torch.Size([50, 13824]) from checkpoin
t, the shape in current model is torch.Size([16384, 13824]).
        size mismatch for cls_head.0.bias: copying a param with shape torch.Size([50]) from checkpoint, the sh
ape in current model is torch.Size([16384]).
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /workspace/study/permvit/wandb/run-20231102_185213-6crbrrtx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k_jigsaw_small_patch56_336_e30_c1000frcl50
wandb: ⭐️ View project at https://wandb.ai/doem97/Puzzle
wandb: 🚀 View run at https://wandb.ai/doem97/Puzzle/runs/6crbrrtx
Namespace(batch_size=128, epochs=50, bce_loss=True, unscale_lr=True, rec=False, freeze=True, model='jigsaw_sma
ll_patch56_336', input_size=336, permcls=1000, drop=0.0, drop_path=0.1, model_ema=True, model_ema_decay=0.9999
6, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight
_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, mi
n_lr=1e-08, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_ji
tter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=None, train_interpolation='bicubic', repeated_aug=True, train_m
ode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cut
mix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety
_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='./ou
tputs/in1k_jigsaw_small_patch56_336_e30_c1000/best_checkpoint.pth', attn_only=False, data_path='/workspace/dat
a/imagenet/ILSVRC/Data/CLS-LOC', data_set='IMNET', nb_classes=1000, inat_category='name', output_dir='./output
s/in1k_jigsaw_small_patch56_336_e30_c1000frcl50', device='cuda', seed=0, resume='', start_epoch=0, eval=False,
 eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=8, dist_url='env://', local_
rank=0, use_jigsaw=True, use_cls=True, lambda_rec=0.1, mask_ratio=0.0, rank=0, gpu=0, distributed=True, dist_b
ackend='nccl')
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 21163 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 21164) of binary:
 /opt/conda/envs/study/bin/python
Traceback (most recent call last):
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __
call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in la
unch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
main_jigsaw.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-11-02_18:52:25
  host      : e20a9f9e5a00
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 21165)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-11-02_18:52:25
  host      : e20a9f9e5a00
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 21166)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-11-02_18:52:25
  host      : e20a9f9e5a00
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 21167)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-11-02_18:52:25
  host      : e20a9f9e5a00
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 21168)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-11-02_18:52:25
  host      : e20a9f9e5a00
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 21169)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-11-02_18:52:25
  host      : e20a9f9e5a00
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 21170)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-02_18:52:25
  host      : e20a9f9e5a00
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 21164)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(study) root@e20a9f9e5a00:/workspace/study/permvit# bash ./exps/dgx/jigsaw_small_p56_336_in1k_c1000frcl50.sh
/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module
torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See
https://pytorch.org/docs/stable/distributed.html#launch-utility for
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being o
verloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
| distributed init (rank 5): env://
| distributed init (rank 7): env://
| distributed init (rank 1): env://
| distributed init (rank 4): env://
| distributed init (rank 3): env://
| distributed init (rank 6): env://
| distributed init (rank 2): env://
| distributed init (rank 0): env://
wandb: Currently logged in as: doem97. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 981, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 698, in main
    and checkpoint_model[k].shape != state_dict[k].shape
KeyError: 'cls_head.1.weight'
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 981, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 698, in main
    and checkpoint_model[k].shape != state_dict[k].shape
KeyError: 'cls_head.1.weight'
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 981, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 698, in main
    and checkpoint_model[k].shape != state_dict[k].shape
KeyError: 'cls_head.1.weight'
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 981, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 698, in main
Traceback (most recent call last):
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 981, in <module>
  File "/workspace/study/permvit/main_jigsaw.py", line 981, in <module>
    and checkpoint_model[k].shape != state_dict[k].shape
KeyError: 'cls_head.1.weight'
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 698, in main
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 698, in main
    and checkpoint_model[k].shape != state_dict[k].shape
KeyError    : and checkpoint_model[k].shape != state_dict[k].shape'cls_head.1.weight'

KeyError: 'cls_head.1.weight'
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 981, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 698, in main
    and checkpoint_model[k].shape != state_dict[k].shape
KeyError: 'cls_head.1.weight'
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /workspace/study/permvit/wandb/run-20231102_185617-ib1ssm4h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k_jigsaw_small_patch56_336_e30_c1000frcl50
wandb: ⭐️ View project at https://wandb.ai/doem97/Puzzle
wandb: 🚀 View run at https://wandb.ai/doem97/Puzzle/runs/ib1ssm4h
Namespace(batch_size=128, epochs=50, bce_loss=True, unscale_lr=True, rec=False, freeze=True, model='jigsaw_sma
ll_patch56_336', input_size=336, permcls=1000, drop=0.0, drop_path=0.1, model_ema=True, model_ema_decay=0.9999
6, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight
_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, mi
n_lr=1e-08, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_ji
tter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=None, train_interpolation='bicubic', repeated_aug=True, train_m
ode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cut
mix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety
_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='./ou
tputs/in1k_jigsaw_small_patch56_336_e30_c1000/best_checkpoint.pth', attn_only=False, data_path='/workspace/dat
a/imagenet/ILSVRC/Data/CLS-LOC', data_set='IMNET', nb_classes=1000, inat_category='name', output_dir='./output
s/in1k_jigsaw_small_patch56_336_e30_c1000frcl50', device='cuda', seed=0, resume='', start_epoch=0, eval=False,
 eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=8, dist_url='env://', local_
rank=0, use_jigsaw=True, use_cls=True, lambda_rec=0.1, mask_ratio=0.0, rank=0, gpu=0, distributed=True, dist_b
ackend='nccl')
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 23734 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 23735) of binary:
 /opt/conda/envs/study/bin/python
Traceback (most recent call last):
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __
call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in la
unch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
main_jigsaw.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-11-02_18:56:28
  host      : e20a9f9e5a00
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 23736)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-11-02_18:56:28
  host      : e20a9f9e5a00
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 23737)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-11-02_18:56:28
  host      : e20a9f9e5a00
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 23738)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-11-02_18:56:28
  host      : e20a9f9e5a00
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 23739)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-11-02_18:56:28
  host      : e20a9f9e5a00
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 23740)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-11-02_18:56:28
  host      : e20a9f9e5a00
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 23741)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-02_18:56:28
  host      : e20a9f9e5a00
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 23735)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(study) root@e20a9f9e5a00:/workspace/study/permvit# bash ./exps/dgx/jigsaw_small_p56_336_in1k_c1000frcl50.sh
/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module
torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See
https://pytorch.org/docs/stable/distributed.html#launch-utility for
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being o
verloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
| distributed init (rank 5): env://
| distributed init (rank 0): env://
| distributed init (rank 1): env://
| distributed init (rank 4): env://
| distributed init (rank 6): env://
| distributed init (rank 3): env://
| distributed init (rank 2): env://
| distributed init (rank 7): env://
wandb: Currently logged in as: doem97. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /workspace/study/permvit/wandb/run-20231102_185706-ycu6guy2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k_jigsaw_small_patch56_336_e30_c1000frcl50
wandb: ⭐️ View project at https://wandb.ai/doem97/Puzzle
wandb: 🚀 View run at https://wandb.ai/doem97/Puzzle/runs/ycu6guy2
Namespace(batch_size=128, epochs=50, bce_loss=True, unscale_lr=True, rec=False, freeze=True, model='jigsaw_sma
ll_patch56_336', input_size=336, permcls=1000, drop=0.0, drop_path=0.1, model_ema=True, model_ema_decay=0.9999
6, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight
_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, mi
n_lr=1e-08, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_ji
tter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=None, train_interpolation='bicubic', repeated_aug=True, train_m
ode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cut
mix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety
_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='./ou
tputs/in1k_jigsaw_small_patch56_336_e30_c1000/best_checkpoint.pth', attn_only=False, data_path='/workspace/dat
a/imagenet/ILSVRC/Data/CLS-LOC', data_set='IMNET', nb_classes=1000, inat_category='name', output_dir='./output
s/in1k_jigsaw_small_patch56_336_e30_c1000frcl50', device='cuda', seed=0, resume='', start_epoch=0, eval=False,
 eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=8, dist_url='env://', local_
rank=0, use_jigsaw=True, use_cls=True, lambda_rec=0.1, mask_ratio=0.0, rank=0, gpu=0, distributed=True, dist_b
ackend='nccl')
Loading classification branch: True
Loading classification branch: True
Creating model: jigsaw_small_patch56_336
Removing key cls_head.0.weight from pretrained checkpoint
Removing key cls_head.0.bias from pretrained checkpoint
number of params: 319077484
using bce loss
Start training for 50 epochs
Epoch: [0]  [0/2]  eta: 0:00:17  lr: 0.001000  loss_total: 4.3412 (4.3412)  loss_cls: 4.3412 (4.3412)  acc1_cl
s: 2.3438 (2.3438)  acc5_cls: 10.1562 (10.1562)  time: 8.5362  data: 5.0771  max mem: 11227
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 2: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 1: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 7: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 5: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 4: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 3: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
Traceback (most recent call last):
  File "/workspace/study/permvit/main_jigsaw.py", line 979, in <module>
    main(args)
  File "/workspace/study/permvit/main_jigsaw.py", line 866, in main
    train_stats = train_one_epoch_cls(
  File "/workspace/study/permvit/engine_jigsaw.py", line 169, in train_one_epoch_cls
    output = model(images, targets, my_images)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_im
pl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1026, in fo
rward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error
 indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters
=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fi
x this by making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output
 tensors in the return value of your module's `forward` function. Please include the loss function and the str
ucture of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 6: 156 157
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print o
ut information about which particular parameters did not receive gradient on this rank as part of this error
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 24403 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 24404) of binary:
 /opt/conda/envs/study/bin/python
Traceback (most recent call last):
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/study/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __
call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in la
unch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
main_jigsaw.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-11-02_18:57:37
  host      : e20a9f9e5a00
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 24405)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-11-02_18:57:37
  host      : e20a9f9e5a00
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 24406)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-11-02_18:57:37
  host      : e20a9f9e5a00
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 24407)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-11-02_18:57:37
  host      : e20a9f9e5a00
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 24408)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-11-02_18:57:37
  host      : e20a9f9e5a00
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 24409)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-11-02_18:57:37
  host      : e20a9f9e5a00
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 24410)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-02_18:57:37
  host      : e20a9f9e5a00
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 24404)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(study) root@e20a9f9e5a00:/workspace/study/permvit# bash ./exps/dgx/jigsaw_small_p56_336_in1k_c1000frcl50.sh
/opt/conda/envs/study/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module
torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See
https://pytorch.org/docs/stable/distributed.html#launch-utility for
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being o
verloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
| distributed init (rank 4): env://
| distributed init (rank 1): env://
| distributed init (rank 3): env://
| distributed init (rank 5): env://
| distributed init (rank 0): env://
| distributed init (rank 6): env://
| distributed init (rank 2): env://
| distributed init (rank 7): env://
wandb: Currently logged in as: doem97. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /workspace/study/permvit/wandb/run-20231102_185907-6wiferil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run in1k_jigsaw_small_patch56_336_e30_c1000frcl50
wandb: ⭐️ View project at https://wandb.ai/doem97/Puzzle
wandb: 🚀 View run at https://wandb.ai/doem97/Puzzle/runs/6wiferil
Namespace(batch_size=128, epochs=50, bce_loss=True, unscale_lr=True, rec=False, freeze=True, model='jigsaw_sma
ll_patch56_336', input_size=336, permcls=1000, drop=0.0, drop_path=0.1, model_ema=True, model_ema_decay=0.9999
6, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight
_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, mi
n_lr=1e-08, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_ji
tter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=None, train_interpolation='bicubic', repeated_aug=True, train_m
ode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cut
mix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety
_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='./ou
tputs/in1k_jigsaw_small_patch56_336_e30_c1000/best_checkpoint.pth', attn_only=False, data_path='/workspace/dat
a/imagenet/ILSVRC/Data/CLS-LOC', data_set='IMNET', nb_classes=1000, inat_category='name', output_dir='./output
s/in1k_jigsaw_small_patch56_336_e30_c1000frcl50', device='cuda', seed=0, resume='', start_epoch=0, eval=False,
 eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, world_size=8, dist_url='env://', local_
rank=0, use_jigsaw=True, use_cls=True, lambda_rec=0.1, mask_ratio=0.0, rank=0, gpu=0, distributed=True, dist_b
ackend='nccl')
Loading classification branch: True
Loading classification branch: True
Creating model: jigsaw_small_patch56_336
Removing key cls_head.0.weight from pretrained checkpoint
Removing key cls_head.0.bias from pretrained checkpoint
number of params: 319077484
using bce loss
Start training for 50 epochs
Epoch: [0]  [0/2]  eta: 0:00:14  lr: 0.001000  loss_total: 4.4821 (4.4821)  loss_cls: 4.4821 (4.4821)  acc1_cl
s: 1.5625 (1.5625)  acc5_cls: 4.6875 (4.6875)  time: 7.3122  data: 4.3599  max mem: 10232
Epoch: [0]  [1/2]  eta: 0:00:03  lr: 0.001000  loss_total: 3.8189 (4.1505)  loss_cls: 3.8189 (4.1505)  acc1_cl
s: 1.5625 (7.8125)  acc5_cls: 4.6875 (19.1406)  time: 3.7733  data: 2.1800  max mem: 10232
Epoch: [0] Total time: 0:00:07 (3.8750 s / it)
Averaged stats: lr: 0.001000  loss_total: 3.8189 (4.1624)  loss_cls: 3.8189 (4.1624)  acc1_cls: 1.5625 (7.1289
)  acc5_cls: 4.6875 (19.4824)
Test:  [0/4]  eta: 0:00:14  loss: 87.0935 (87.0935)  acc1_cls: 6.2500 (6.2500)  acc5_cls: 7.8125 (7.8125)  tim
e: 3.5677  data: 3.4205  max mem: 10232
Test:  [3/4]  eta: 0:00:01  loss: 77.1488 (79.8643)  acc1_cls: 0.0000 (3.3898)  acc5_cls: 7.8125 (9.9576)  tim
e: 1.0215  data: 0.9275  max mem: 10232
Test: Total time: 0:00:04 (1.0700 s / it)
* Batch-avg:             Cls Acc1: 3.390,                Cls Acc5: 9.958,                loss 79.864
Max acc1_cls: 3.39%
Epoch: [1]  [0/2]  eta: 0:00:08  lr: 0.001000  loss_total: 3.8187 (3.8187)  loss_cls: 3.8187 (3.8187)  acc1_cl
s: 19.5312 (19.5312)  acc5_cls: 32.8125 (32.8125)  time: 4.2986  data: 4.0903  max mem: 10232
Epoch: [1]  [1/2]  eta: 0:00:02  lr: 0.001000  loss_total: 3.4843 (3.6515)  loss_cls: 3.4843 (3.6515)  acc1_cl
s: 19.5312 (21.0938)  acc5_cls: 32.8125 (35.9375)  time: 2.2644  data: 2.0452  max mem: 10232
Epoch: [1] Total time: 0:00:04 (2.4026 s / it)
Averaged stats: lr: 0.001000  loss_total: 3.4843 (3.7496)  loss_cls: 3.4843 (3.7496)  acc1_cls: 19.5312 (17.91
99)  acc5_cls: 32.8125 (33.5449)
Test:  [0/4]  eta: 0:00:13  loss: 43.4722 (43.4722)  acc1_cls: 5.4688 (5.4688)  acc5_cls: 14.8438 (14.8438)  t
ime: 3.4069  data: 3.3529  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 34.9746 (37.4685)  acc1_cls: 3.4091 (4.0254)  acc5_cls: 14.8438 (13.7712)  t
ime: 0.9087  data: 0.8383  max mem: 10232
Test: Total time: 0:00:03 (0.9594 s / it)
* Batch-avg:             Cls Acc1: 4.025,                Cls Acc5: 13.771,                loss 37.469
Max acc1_cls: 4.03%
Epoch: [2]  [0/2]  eta: 0:00:08  lr: 0.000999  loss_total: 3.5526 (3.5526)  loss_cls: 3.5526 (3.5526)  acc1_cl
s: 25.0000 (25.0000)  acc5_cls: 35.1562 (35.1562)  time: 4.0674  data: 3.8644  max mem: 10232
Epoch: [2]  [1/2]  eta: 0:00:02  lr: 0.000999  loss_total: 3.5438 (3.5482)  loss_cls: 3.5438 (3.5482)  acc1_cl
s: 21.8750 (23.4375)  acc5_cls: 35.1562 (38.6719)  time: 2.1484  data: 1.9323  max mem: 10232
Epoch: [2] Total time: 0:00:04 (2.2969 s / it)
Averaged stats: lr: 0.000999  loss_total: 3.5438 (3.5234)  loss_cls: 3.5438 (3.5234)  acc1_cls: 21.8750 (25.09
77)  acc5_cls: 35.1562 (41.1621)
Test:  [0/4]  eta: 0:00:13  loss: 26.9392 (26.9392)  acc1_cls: 1.5625 (1.5625)  acc5_cls: 17.1875 (17.1875)  t
ime: 3.3187  data: 3.2656  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 25.0148 (25.6881)  acc1_cls: 1.5625 (3.1780)  acc5_cls: 11.3636 (13.7712)  t
ime: 0.8868  data: 0.8353  max mem: 10232
Test: Total time: 0:00:03 (0.9601 s / it)
* Batch-avg:             Cls Acc1: 3.178,                Cls Acc5: 13.771,                loss 25.688
Max acc1_cls: 4.03%
Epoch: [3]  [0/2]  eta: 0:00:10  lr: 0.000996  loss_total: 3.4133 (3.4133)  loss_cls: 3.4133 (3.4133)  acc1_cl
s: 26.5625 (26.5625)  acc5_cls: 39.8438 (39.8438)  time: 5.2067  data: 5.0150  max mem: 10232
Epoch: [3]  [1/2]  eta: 0:00:02  lr: 0.000996  loss_total: 3.3268 (3.3701)  loss_cls: 3.3268 (3.3701)  acc1_cl
s: 26.5625 (28.5156)  acc5_cls: 39.8438 (42.9688)  time: 2.7197  data: 2.5075  max mem: 10232
Epoch: [3] Total time: 0:00:05 (2.8621 s / it)
Averaged stats: lr: 0.000996  loss_total: 3.3268 (3.3355)  loss_cls: 3.3268 (3.3355)  acc1_cls: 26.5625 (29.24
80)  acc5_cls: 39.8438 (44.8242)
Test:  [0/4]  eta: 0:00:13  loss: 29.6453 (29.6453)  acc1_cls: 0.0000 (0.0000)  acc5_cls: 4.6875 (4.6875)  tim
e: 3.2589  data: 3.2049  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 24.7725 (26.0884)  acc1_cls: 0.0000 (2.9661)  acc5_cls: 13.2812 (15.2542)  t
ime: 0.8709  data: 0.8013  max mem: 10232
Test: Total time: 0:00:03 (0.9509 s / it)
* Batch-avg:             Cls Acc1: 2.966,                Cls Acc5: 15.254,                loss 26.088
Max acc1_cls: 4.03%
Epoch: [4]  [0/2]  eta: 0:00:11  lr: 0.000991  loss_total: 3.2178 (3.2178)  loss_cls: 3.2178 (3.2178)  acc1_cl
s: 28.9062 (28.9062)  acc5_cls: 46.8750 (46.8750)  time: 5.5699  data: 4.7984  max mem: 10232
Epoch: [4]  [1/2]  eta: 0:00:02  lr: 0.000991  loss_total: 2.9665 (3.0921)  loss_cls: 2.9665 (3.0921)  acc1_cl
s: 28.9062 (32.4219)  acc5_cls: 46.8750 (51.9531)  time: 2.9020  data: 2.3993  max mem: 10232
Epoch: [4] Total time: 0:00:06 (3.0580 s / it)
Averaged stats: lr: 0.000991  loss_total: 2.9665 (3.0433)  loss_cls: 2.9665 (3.0433)  acc1_cls: 28.9062 (33.64
26)  acc5_cls: 46.8750 (53.4668)
Test:  [0/4]  eta: 0:00:12  loss: 17.3748 (17.3748)  acc1_cls: 4.6875 (4.6875)  acc5_cls: 10.1562 (10.1562)  t
ime: 3.1194  data: 3.0647  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 14.6507 (14.2142)  acc1_cls: 4.6875 (6.9915)  acc5_cls: 17.1875 (18.8559)  t
ime: 0.8679  data: 0.8150  max mem: 10232
Test: Total time: 0:00:03 (0.9383 s / it)
* Batch-avg:             Cls Acc1: 6.992,                Cls Acc5: 18.856,                loss 14.214
Max acc1_cls: 6.99%
Epoch: [5]  [0/2]  eta: 0:00:08  lr: 0.000984  loss_total: 3.0219 (3.0219)  loss_cls: 3.0219 (3.0219)  acc1_cl
s: 35.1562 (35.1562)  acc5_cls: 53.1250 (53.1250)  time: 4.3401  data: 4.1442  max mem: 10232
Epoch: [5]  [1/2]  eta: 0:00:02  lr: 0.000984  loss_total: 2.8277 (2.9248)  loss_cls: 2.8277 (2.9248)  acc1_cl
s: 35.1562 (39.4531)  acc5_cls: 53.1250 (56.6406)  time: 2.2861  data: 2.0722  max mem: 10232
Epoch: [5] Total time: 0:00:04 (2.4454 s / it)
Averaged stats: lr: 0.000984  loss_total: 2.8277 (2.9579)  loss_cls: 2.8277 (2.9579)  acc1_cls: 35.1562 (37.79
30)  acc5_cls: 53.1250 (55.6152)
Test:  [0/4]  eta: 0:00:14  loss: 19.0959 (19.0959)  acc1_cls: 0.7812 (0.7812)  acc5_cls: 24.2188 (24.2188)  t
ime: 3.5928  data: 3.5375  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 18.2931 (17.7915)  acc1_cls: 0.7812 (2.3305)  acc5_cls: 20.3125 (23.5169)  t
ime: 0.9547  data: 0.8844  max mem: 10232
Test: Total time: 0:00:04 (1.0550 s / it)
* Batch-avg:             Cls Acc1: 2.331,                Cls Acc5: 23.517,                loss 17.792
Max acc1_cls: 6.99%
Epoch: [6]  [0/2]  eta: 0:00:10  lr: 0.000976  loss_total: 2.7909 (2.7909)  loss_cls: 2.7909 (2.7909)  acc1_cl
s: 43.7500 (43.7500)  acc5_cls: 61.7188 (61.7188)  time: 5.4532  data: 5.0032  max mem: 10232
Epoch: [6]  [1/2]  eta: 0:00:02  lr: 0.000976  loss_total: 2.7909 (2.9028)  loss_cls: 2.7909 (2.9028)  acc1_cl
s: 43.7500 (44.5312)  acc5_cls: 55.4688 (58.5938)  time: 2.8445  data: 2.5016  max mem: 10232
Epoch: [6] Total time: 0:00:05 (2.9840 s / it)
Averaged stats: lr: 0.000976  loss_total: 2.7909 (2.8678)  loss_cls: 2.7909 (2.8678)  acc1_cls: 43.7500 (43.50
59)  acc5_cls: 55.4688 (57.8613)
Test:  [0/4]  eta: 0:00:13  loss: 13.3246 (13.3246)  acc1_cls: 3.1250 (3.1250)  acc5_cls: 20.3125 (20.3125)  t
ime: 3.3854  data: 3.3323  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 13.1632 (12.9975)  acc1_cls: 1.1364 (4.2373)  acc5_cls: 18.7500 (17.1610)  t
ime: 0.9062  data: 0.8545  max mem: 10232
Test: Total time: 0:00:04 (1.0192 s / it)
* Batch-avg:             Cls Acc1: 4.237,                Cls Acc5: 17.161,                loss 12.997
Max acc1_cls: 6.99%
Epoch: [7]  [0/2]  eta: 0:00:11  lr: 0.000965  loss_total: 2.6657 (2.6657)  loss_cls: 2.6657 (2.6657)  acc1_cl
s: 46.0938 (46.0938)  acc5_cls: 62.5000 (62.5000)  time: 5.7510  data: 5.1163  max mem: 10232
Epoch: [7]  [1/2]  eta: 0:00:02  lr: 0.000965  loss_total: 2.6657 (2.6955)  loss_cls: 2.6657 (2.6955)  acc1_cl
s: 45.3125 (45.7031)  acc5_cls: 61.7188 (62.1094)  time: 2.9928  data: 2.5582  max mem: 10232
Epoch: [7] Total time: 0:00:06 (3.1600 s / it)
Averaged stats: lr: 0.000965  loss_total: 2.6657 (2.6782)  loss_cls: 2.6657 (2.6782)  acc1_cls: 45.3125 (44.82
42)  acc5_cls: 61.7188 (61.6699)
Test:  [0/4]  eta: 0:00:14  loss: 8.8192 (8.8192)  acc1_cls: 10.9375 (10.9375)  acc5_cls: 24.2188 (24.2188)  t
ime: 3.6517  data: 3.5985  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 9.1123 (9.6902)  acc1_cls: 5.6818 (7.8390)  acc5_cls: 19.5312 (20.3390)  tim
e: 0.9670  data: 0.9016  max mem: 10232
Test: Total time: 0:00:04 (1.0685 s / it)
* Batch-avg:             Cls Acc1: 7.839,                Cls Acc5: 20.339,                loss 9.690
Max acc1_cls: 7.84%
Epoch: [8]  [0/2]  eta: 0:00:09  lr: 0.000952  loss_total: 2.5798 (2.5798)  loss_cls: 2.5798 (2.5798)  acc1_cl
s: 42.9688 (42.9688)  acc5_cls: 67.1875 (67.1875)  time: 4.5231  data: 4.2922  max mem: 10232
Epoch: [8]  [1/2]  eta: 0:00:02  lr: 0.000952  loss_total: 2.2673 (2.4236)  loss_cls: 2.2673 (2.4236)  acc1_cl
s: 42.9688 (52.3438)  acc5_cls: 67.1875 (71.8750)  time: 2.3609  data: 2.1462  max mem: 10232
Epoch: [8] Total time: 0:00:04 (2.4486 s / it)
Averaged stats: lr: 0.000952  loss_total: 2.2673 (2.4299)  loss_cls: 2.2673 (2.4299)  acc1_cls: 42.9688 (51.26
95)  acc5_cls: 67.1875 (71.2891)
Test:  [0/4]  eta: 0:00:14  loss: 7.3437 (7.3437)  acc1_cls: 12.5000 (12.5000)  acc5_cls: 29.6875 (29.6875)  t
ime: 3.5105  data: 3.4567  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 8.7312 (8.7318)  acc1_cls: 10.1562 (9.1102)  acc5_cls: 16.4062 (21.1864)  ti
me: 0.9367  data: 0.8850  max mem: 10232
Test: Total time: 0:00:04 (1.0480 s / it)
* Batch-avg:             Cls Acc1: 9.110,                Cls Acc5: 21.186,                loss 8.732
Max acc1_cls: 9.11%
Epoch: [9]  [0/2]  eta: 0:00:09  lr: 0.000938  loss_total: 2.3244 (2.3244)  loss_cls: 2.3244 (2.3244)  acc1_cl
s: 50.7812 (50.7812)  acc5_cls: 71.0938 (71.0938)  time: 4.9119  data: 4.7170  max mem: 10232
Epoch: [9]  [1/2]  eta: 0:00:02  lr: 0.000938  loss_total: 2.1718 (2.2481)  loss_cls: 2.1718 (2.2481)  acc1_cl
s: 50.7812 (56.6406)  acc5_cls: 71.0938 (72.2656)  time: 2.5739  data: 2.3585  max mem: 10232
Epoch: [9] Total time: 0:00:05 (2.7405 s / it)
Averaged stats: lr: 0.000938  loss_total: 2.1718 (2.2325)  loss_cls: 2.1718 (2.2325)  acc1_cls: 50.7812 (56.78
71)  acc5_cls: 71.0938 (74.8535)
Test:  [0/4]  eta: 0:00:14  loss: 5.3345 (5.3345)  acc1_cls: 15.6250 (15.6250)  acc5_cls: 35.1562 (35.1562)  t
ime: 3.7158  data: 3.6607  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 6.4710 (6.6463)  acc1_cls: 13.6364 (13.9831)  acc5_cls: 27.3438 (30.2966)  t
ime: 0.9844  data: 0.9152  max mem: 10232
Test: Total time: 0:00:04 (1.0905 s / it)
* Batch-avg:             Cls Acc1: 13.983,                Cls Acc5: 30.297,                loss 6.646
Max acc1_cls: 13.98%
Epoch: [10]  [0/2]  eta: 0:00:09  lr: 0.000922  loss_total: 2.1103 (2.1103)  loss_cls: 2.1103 (2.1103)  acc1_c
ls: 63.2812 (63.2812)  acc5_cls: 78.9062 (78.9062)  time: 4.6595  data: 4.4654  max mem: 10232
Epoch: [10]  [1/2]  eta: 0:00:02  lr: 0.000922  loss_total: 2.0458 (2.0780)  loss_cls: 2.0458 (2.0780)  acc1_c
ls: 59.3750 (61.3281)  acc5_cls: 78.1250 (78.5156)  time: 2.4504  data: 2.2327  max mem: 10232
Epoch: [10] Total time: 0:00:05 (2.6074 s / it)
Averaged stats: lr: 0.000922  loss_total: 2.0458 (2.0617)  loss_cls: 2.0458 (2.0617)  acc1_cls: 59.3750 (62.98
83)  acc5_cls: 78.1250 (78.9062)
Test:  [0/4]  eta: 0:00:15  loss: 4.4272 (4.4272)  acc1_cls: 17.9688 (17.9688)  acc5_cls: 46.0938 (46.0938)  t
ime: 3.8997  data: 3.8446  max mem: 10232
Test:  [3/4]  eta: 0:00:01  loss: 4.8045 (5.4683)  acc1_cls: 17.1875 (16.1017)  acc5_cls: 36.7188 (38.1356)  t
ime: 1.0305  data: 0.9612  max mem: 10232
Test: Total time: 0:00:04 (1.1305 s / it)
* Batch-avg:             Cls Acc1: 16.102,                Cls Acc5: 38.136,                loss 5.468
Max acc1_cls: 16.10%
Epoch: [11]  [0/2]  eta: 0:00:09  lr: 0.000905  loss_total: 2.1454 (2.1454)  loss_cls: 2.1454 (2.1454)  acc1_c
ls: 58.5938 (58.5938)  acc5_cls: 75.7812 (75.7812)  time: 4.7947  data: 4.6044  max mem: 10232
Epoch: [11]  [1/2]  eta: 0:00:02  lr: 0.000905  loss_total: 2.0250 (2.0852)  loss_cls: 2.0250 (2.0852)  acc1_c
ls: 58.5938 (61.3281)  acc5_cls: 75.7812 (78.1250)  time: 2.5331  data: 2.3022  max mem: 10232
Epoch: [11] Total time: 0:00:05 (2.7400 s / it)
Averaged stats: lr: 0.000905  loss_total: 2.0250 (2.0296)  loss_cls: 2.0250 (2.0296)  acc1_cls: 58.5938 (63.76
95)  acc5_cls: 75.7812 (78.7109)
Test:  [0/4]  eta: 0:00:14  loss: 4.1213 (4.1213)  acc1_cls: 25.7812 (25.7812)  acc5_cls: 46.8750 (46.8750)  t
ime: 3.5781  data: 3.5235  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 4.1213 (4.8587)  acc1_cls: 18.7500 (18.8559)  acc5_cls: 42.1875 (40.8898)  t
ime: 0.9994  data: 0.9518  max mem: 10232
Test: Total time: 0:00:04 (1.0497 s / it)
* Batch-avg:             Cls Acc1: 18.856,                Cls Acc5: 40.890,                loss 4.859
Max acc1_cls: 18.86%
Epoch: [12]  [0/2]  eta: 0:00:09  lr: 0.000885  loss_total: 1.8052 (1.8052)  loss_cls: 1.8052 (1.8052)  acc1_c
ls: 69.5312 (69.5312)  acc5_cls: 83.5938 (83.5938)  time: 4.5899  data: 4.3658  max mem: 10232
Epoch: [12]  [1/2]  eta: 0:00:02  lr: 0.000885  loss_total: 1.8052 (1.8087)  loss_cls: 1.8052 (1.8087)  acc1_c
ls: 65.6250 (67.5781)  acc5_cls: 83.5938 (83.5938)  time: 2.4129  data: 2.1830  max mem: 10232
Epoch: [12] Total time: 0:00:05 (2.5551 s / it)
Averaged stats: lr: 0.000885  loss_total: 1.8052 (1.7997)  loss_cls: 1.8052 (1.7997)  acc1_cls: 65.6250 (69.92
19)  acc5_cls: 83.5938 (83.3984)
Test:  [0/4]  eta: 0:00:13  loss: 4.3309 (4.3309)  acc1_cls: 21.8750 (21.8750)  acc5_cls: 39.8438 (39.8438)  t
ime: 3.4534  data: 3.3992  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 4.3309 (4.7163)  acc1_cls: 18.7500 (19.9153)  acc5_cls: 35.9375 (36.6525)  t
ime: 0.9566  data: 0.9031  max mem: 10232
Test: Total time: 0:00:04 (1.0204 s / it)
* Batch-avg:             Cls Acc1: 19.915,                Cls Acc5: 36.653,                loss 4.716
Max acc1_cls: 19.92%
Epoch: [13]  [0/2]  eta: 0:00:09  lr: 0.000864  loss_total: 1.7594 (1.7594)  loss_cls: 1.7594 (1.7594)  acc1_c
ls: 73.4375 (73.4375)  acc5_cls: 83.5938 (83.5938)  time: 4.7599  data: 4.5629  max mem: 10232
Epoch: [13]  [1/2]  eta: 0:00:02  lr: 0.000864  loss_total: 1.7530 (1.7562)  loss_cls: 1.7530 (1.7562)  acc1_c
ls: 69.5312 (71.4844)  acc5_cls: 81.2500 (82.4219)  time: 2.4970  data: 2.2815  max mem: 10232
Epoch: [13] Total time: 0:00:05 (2.6747 s / it)
Averaged stats: lr: 0.000864  loss_total: 1.7530 (1.7682)  loss_cls: 1.7530 (1.7682)  acc1_cls: 69.5312 (71.33
79)  acc5_cls: 81.2500 (83.6914)
Test:  [0/4]  eta: 0:00:13  loss: 4.4752 (4.4752)  acc1_cls: 18.7500 (18.7500)  acc5_cls: 39.8438 (39.8438)  t
ime: 3.4955  data: 3.4398  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 4.4752 (4.5262)  acc1_cls: 18.7500 (18.0085)  acc5_cls: 35.9375 (39.6186)  t
ime: 0.9319  data: 0.8696  max mem: 10232
Test: Total time: 0:00:04 (1.0317 s / it)
* Batch-avg:             Cls Acc1: 18.008,                Cls Acc5: 39.619,                loss 4.526
Max acc1_cls: 19.92%
Epoch: [14]  [0/2]  eta: 0:00:11  lr: 0.000842  loss_total: 1.8327 (1.8327)  loss_cls: 1.8327 (1.8327)  acc1_c
ls: 71.0938 (71.0938)  acc5_cls: 82.0312 (82.0312)  time: 5.5329  data: 4.9895  max mem: 10232
Epoch: [14]  [1/2]  eta: 0:00:02  lr: 0.000842  loss_total: 1.7558 (1.7943)  loss_cls: 1.7558 (1.7943)  acc1_c
ls: 68.7500 (69.9219)  acc5_cls: 82.0312 (82.4219)  time: 2.8820  data: 2.4948  max mem: 10232
Epoch: [14] Total time: 0:00:06 (3.0464 s / it)
Averaged stats: lr: 0.000842  loss_total: 1.7558 (1.7469)  loss_cls: 1.7558 (1.7469)  acc1_cls: 68.7500 (70.31
25)  acc5_cls: 82.0312 (84.0332)
Test:  [0/4]  eta: 0:00:14  loss: 4.1308 (4.1308)  acc1_cls: 19.5312 (19.5312)  acc5_cls: 51.5625 (51.5625)  t
ime: 3.6755  data: 3.6210  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 4.1308 (4.1362)  acc1_cls: 19.5312 (19.2797)  acc5_cls: 39.8438 (45.1271)  t
ime: 0.9703  data: 0.9114  max mem: 10232
Test: Total time: 0:00:04 (1.0451 s / it)
* Batch-avg:             Cls Acc1: 19.280,                Cls Acc5: 45.127,                loss 4.136
Max acc1_cls: 19.92%
Epoch: [15]  [0/2]  eta: 0:00:10  lr: 0.000819  loss_total: 1.4526 (1.4526)  loss_cls: 1.4526 (1.4526)  acc1_c
ls: 79.6875 (79.6875)  acc5_cls: 87.5000 (87.5000)  time: 5.4433  data: 5.2535  max mem: 10232
Epoch: [15]  [1/2]  eta: 0:00:02  lr: 0.000819  loss_total: 1.4526 (1.5041)  loss_cls: 1.4526 (1.5041)  acc1_c
ls: 71.8750 (75.7812)  acc5_cls: 87.5000 (88.6719)  time: 2.8372  data: 2.6268  max mem: 10232
Epoch: [15] Total time: 0:00:05 (2.9998 s / it)
Averaged stats: lr: 0.000819  loss_total: 1.4526 (1.5511)  loss_cls: 1.4526 (1.5511)  acc1_cls: 71.8750 (75.39
06)  acc5_cls: 87.5000 (87.5977)
Test:  [0/4]  eta: 0:00:15  loss: 4.1610 (4.1610)  acc1_cls: 16.4062 (16.4062)  acc5_cls: 49.2188 (49.2188)  t
ime: 3.8336  data: 3.7784  max mem: 10232
Test:  [3/4]  eta: 0:00:01  loss: 4.1140 (3.9873)  acc1_cls: 16.4062 (17.1610)  acc5_cls: 46.0938 (48.7288)  t
ime: 1.0150  data: 0.9447  max mem: 10232
Test: Total time: 0:00:04 (1.1073 s / it)
* Batch-avg:             Cls Acc1: 17.161,                Cls Acc5: 48.729,                loss 3.987
Max acc1_cls: 19.92%
Epoch: [16]  [0/2]  eta: 0:00:10  lr: 0.000794  loss_total: 1.5469 (1.5469)  loss_cls: 1.5469 (1.5469)  acc1_c
ls: 76.5625 (76.5625)  acc5_cls: 89.8438 (89.8438)  time: 5.4778  data: 4.7979  max mem: 10232
Epoch: [16]  [1/2]  eta: 0:00:02  lr: 0.000794  loss_total: 1.5469 (1.6541)  loss_cls: 1.5469 (1.6541)  acc1_c
ls: 66.4062 (71.4844)  acc5_cls: 82.0312 (85.9375)  time: 2.9350  data: 2.3990  max mem: 10232
Epoch: [16] Total time: 0:00:06 (3.0692 s / it)
Averaged stats: lr: 0.000794  loss_total: 1.5469 (1.5604)  loss_cls: 1.5469 (1.5604)  acc1_cls: 66.4062 (74.70
70)  acc5_cls: 82.0312 (88.1348)
Test:  [0/4]  eta: 0:00:14  loss: 3.9254 (3.9254)  acc1_cls: 16.4062 (16.4062)  acc5_cls: 47.6562 (47.6562)  t
ime: 3.5225  data: 3.4674  max mem: 10232
Test:  [3/4]  eta: 0:00:01  loss: 3.9254 (3.7782)  acc1_cls: 16.4062 (15.6780)  acc5_cls: 49.2188 (53.3898)  t
ime: 1.0124  data: 0.9621  max mem: 10232
Test: Total time: 0:00:04 (1.0594 s / it)
* Batch-avg:             Cls Acc1: 15.678,                Cls Acc5: 53.390,                loss 3.778
Max acc1_cls: 19.92%
Epoch: [17]  [0/2]  eta: 0:00:10  lr: 0.000768  loss_total: 1.6059 (1.6059)  loss_cls: 1.6059 (1.6059)  acc1_c
ls: 75.7812 (75.7812)  acc5_cls: 88.2812 (88.2812)  time: 5.3903  data: 5.1952  max mem: 10232
Epoch: [17]  [1/2]  eta: 0:00:02  lr: 0.000768  loss_total: 1.2858 (1.4458)  loss_cls: 1.2858 (1.4458)  acc1_c
ls: 75.7812 (78.1250)  acc5_cls: 88.2812 (92.5781)  time: 2.8236  data: 2.5976  max mem: 10232
Epoch: [17] Total time: 0:00:05 (2.9674 s / it)
Averaged stats: lr: 0.000768  loss_total: 1.2858 (1.3586)  loss_cls: 1.2858 (1.3586)  acc1_cls: 75.7812 (80.56
64)  acc5_cls: 88.2812 (92.2363)
Test:  [0/4]  eta: 0:00:13  loss: 3.5294 (3.5294)  acc1_cls: 21.0938 (21.0938)  acc5_cls: 58.5938 (58.5938)  t
ime: 3.4893  data: 3.4334  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 3.5294 (3.4625)  acc1_cls: 19.5312 (19.2797)  acc5_cls: 58.5938 (59.5339)  t
ime: 0.9692  data: 0.9149  max mem: 10232
Test: Total time: 0:00:04 (1.0304 s / it)
* Batch-avg:             Cls Acc1: 19.280,                Cls Acc5: 59.534,                loss 3.463
Max acc1_cls: 19.92%
Epoch: [18]  [0/2]  eta: 0:00:11  lr: 0.000741  loss_total: 1.3584 (1.3584)  loss_cls: 1.3584 (1.3584)  acc1_c
ls: 83.5938 (83.5938)  acc5_cls: 92.1875 (92.1875)  time: 5.6065  data: 5.1319  max mem: 10232
Epoch: [18]  [1/2]  eta: 0:00:02  lr: 0.000741  loss_total: 1.2280 (1.2932)  loss_cls: 1.2280 (1.2932)  acc1_c
ls: 83.5938 (84.3750)  acc5_cls: 92.1875 (92.5781)  time: 2.9200  data: 2.5660  max mem: 10232
Epoch: [18] Total time: 0:00:06 (3.0538 s / it)
Averaged stats: lr: 0.000741  loss_total: 1.2280 (1.2917)  loss_cls: 1.2280 (1.2917)  acc1_cls: 83.5938 (82.71
48)  acc5_cls: 92.1875 (90.9180)
Test:  [0/4]  eta: 0:00:15  loss: 3.2074 (3.2074)  acc1_cls: 21.8750 (21.8750)  acc5_cls: 64.8438 (64.8438)  t
ime: 3.9577  data: 3.9045  max mem: 10232
Test:  [3/4]  eta: 0:00:01  loss: 3.1779 (3.1714)  acc1_cls: 21.8750 (22.4576)  acc5_cls: 63.2812 (63.5593)  t
ime: 1.0457  data: 0.9762  max mem: 10232
Test: Total time: 0:00:04 (1.1749 s / it)
* Batch-avg:             Cls Acc1: 22.458,                Cls Acc5: 63.559,                loss 3.171
Max acc1_cls: 22.46%
Epoch: [19]  [0/2]  eta: 0:00:10  lr: 0.000713  loss_total: 1.2428 (1.2428)  loss_cls: 1.2428 (1.2428)  acc1_c
ls: 84.3750 (84.3750)  acc5_cls: 94.5312 (94.5312)  time: 5.0769  data: 4.8663  max mem: 10232
Epoch: [19]  [1/2]  eta: 0:00:02  lr: 0.000713  loss_total: 1.1471 (1.1950)  loss_cls: 1.1471 (1.1950)  acc1_c
ls: 84.3750 (86.3281)  acc5_cls: 93.7500 (94.1406)  time: 2.6544  data: 2.4332  max mem: 10232
Epoch: [19] Total time: 0:00:05 (2.7855 s / it)
Averaged stats: lr: 0.000713  loss_total: 1.1471 (1.1994)  loss_cls: 1.1471 (1.1994)  acc1_cls: 84.3750 (83.88
67)  acc5_cls: 93.7500 (92.8223)
Test:  [0/4]  eta: 0:00:14  loss: 2.8647 (2.8647)  acc1_cls: 27.3438 (27.3438)  acc5_cls: 66.4062 (66.4062)  t
ime: 3.5300  data: 3.4753  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.8647 (2.8872)  acc1_cls: 27.3438 (26.6949)  acc5_cls: 66.4062 (66.5254)  t
ime: 0.9459  data: 0.8918  max mem: 10232
Test: Total time: 0:00:04 (1.0626 s / it)
* Batch-avg:             Cls Acc1: 26.695,                Cls Acc5: 66.525,                loss 2.887
Max acc1_cls: 26.69%
Epoch: [20]  [0/2]  eta: 0:00:09  lr: 0.000684  loss_total: 1.1303 (1.1303)  loss_cls: 1.1303 (1.1303)  acc1_c
ls: 87.5000 (87.5000)  acc5_cls: 93.7500 (93.7500)  time: 4.7758  data: 4.5274  max mem: 10232
Epoch: [20]  [1/2]  eta: 0:00:02  lr: 0.000684  loss_total: 1.1303 (1.1623)  loss_cls: 1.1303 (1.1623)  acc1_c
ls: 81.2500 (84.3750)  acc5_cls: 93.7500 (94.5312)  time: 2.4844  data: 2.2637  max mem: 10232
Epoch: [20] Total time: 0:00:05 (2.6565 s / it)
Averaged stats: lr: 0.000684  loss_total: 1.1303 (1.1505)  loss_cls: 1.1303 (1.1505)  acc1_cls: 81.2500 (85.30
27)  acc5_cls: 93.7500 (94.2871)
Test:  [0/4]  eta: 0:00:13  loss: 2.5959 (2.5959)  acc1_cls: 30.4688 (30.4688)  acc5_cls: 71.0938 (71.0938)  t
ime: 3.3058  data: 3.2486  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.5959 (2.5936)  acc1_cls: 30.4688 (31.7797)  acc5_cls: 71.0938 (71.1864)  t
ime: 0.9629  data: 0.9136  max mem: 10232
Test: Total time: 0:00:04 (1.0172 s / it)
* Batch-avg:             Cls Acc1: 31.780,                Cls Acc5: 71.186,                loss 2.594
Max acc1_cls: 31.78%
Epoch: [21]  [0/2]  eta: 0:00:08  lr: 0.000655  loss_total: 1.0926 (1.0926)  loss_cls: 1.0926 (1.0926)  acc1_c
ls: 89.0625 (89.0625)  acc5_cls: 93.7500 (93.7500)  time: 4.1587  data: 3.9531  max mem: 10232
Epoch: [21]  [1/2]  eta: 0:00:02  lr: 0.000655  loss_total: 1.0926 (1.1025)  loss_cls: 1.0926 (1.1025)  acc1_c
ls: 88.2812 (88.6719)  acc5_cls: 92.9688 (93.3594)  time: 2.1917  data: 1.9766  max mem: 10232
Epoch: [21] Total time: 0:00:04 (2.3553 s / it)
Averaged stats: lr: 0.000655  loss_total: 1.0926 (1.0825)  loss_cls: 1.0926 (1.0825)  acc1_cls: 88.2812 (86.81
64)  acc5_cls: 92.9688 (94.2871)
Test:  [0/4]  eta: 0:00:12  loss: 2.3632 (2.3632)  acc1_cls: 33.5938 (33.5938)  acc5_cls: 72.6562 (72.6562)  t
ime: 3.2265  data: 3.1719  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.3632 (2.3662)  acc1_cls: 33.5938 (35.3814)  acc5_cls: 72.6562 (73.9407)  t
ime: 0.9264  data: 0.8775  max mem: 10232
Test: Total time: 0:00:03 (0.9745 s / it)
* Batch-avg:             Cls Acc1: 35.381,                Cls Acc5: 73.941,                loss 2.366
Max acc1_cls: 35.38%
Epoch: [22]  [0/2]  eta: 0:00:08  lr: 0.000624  loss_total: 1.0317 (1.0317)  loss_cls: 1.0317 (1.0317)  acc1_c
ls: 92.1875 (92.1875)  acc5_cls: 96.8750 (96.8750)  time: 4.4315  data: 4.2388  max mem: 10232
Epoch: [22]  [1/2]  eta: 0:00:02  lr: 0.000624  loss_total: 0.9991 (1.0154)  loss_cls: 0.9991 (1.0154)  acc1_c
ls: 90.6250 (91.4062)  acc5_cls: 96.8750 (96.8750)  time: 2.3334  data: 2.1194  max mem: 10232
Epoch: [22] Total time: 0:00:04 (2.4944 s / it)
Averaged stats: lr: 0.000624  loss_total: 0.9991 (1.0124)  loss_cls: 0.9991 (1.0124)  acc1_cls: 90.6250 (88.76
95)  acc5_cls: 96.8750 (95.8008)
Test:  [0/4]  eta: 0:00:12  loss: 2.2110 (2.2110)  acc1_cls: 41.4062 (41.4062)  acc5_cls: 77.3438 (77.3438)  t
ime: 3.2030  data: 3.1486  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.2086 (2.1971)  acc1_cls: 40.6250 (41.1017)  acc5_cls: 75.0000 (75.8475)  t
ime: 0.8950  data: 0.8475  max mem: 10232
Test: Total time: 0:00:03 (0.9683 s / it)
* Batch-avg:             Cls Acc1: 41.102,                Cls Acc5: 75.847,                loss 2.197
Max acc1_cls: 41.10%
Epoch: [23]  [0/2]  eta: 0:00:08  lr: 0.000594  loss_total: 0.8412 (0.8412)  loss_cls: 0.8412 (0.8412)  acc1_c
ls: 91.4062 (91.4062)  acc5_cls: 97.6562 (97.6562)  time: 4.4823  data: 4.2663  max mem: 10232
Epoch: [23]  [1/2]  eta: 0:00:02  lr: 0.000594  loss_total: 0.8412 (0.9664)  loss_cls: 0.8412 (0.9664)  acc1_c
ls: 86.7188 (89.0625)  acc5_cls: 95.3125 (96.4844)  time: 2.3524  data: 2.1332  max mem: 10232
Epoch: [23] Total time: 0:00:05 (2.5133 s / it)
Averaged stats: lr: 0.000594  loss_total: 0.8412 (0.9502)  loss_cls: 0.8412 (0.9502)  acc1_cls: 86.7188 (89.94
14)  acc5_cls: 95.3125 (96.1914)
Test:  [0/4]  eta: 0:00:13  loss: 2.1462 (2.1462)  acc1_cls: 42.1875 (42.1875)  acc5_cls: 78.9062 (78.9062)  t
ime: 3.4229  data: 3.3681  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.1462 (2.1493)  acc1_cls: 40.6250 (41.9492)  acc5_cls: 78.9062 (76.4831)  t
ime: 0.9113  data: 0.8421  max mem: 10232
Test: Total time: 0:00:03 (0.9988 s / it)
* Batch-avg:             Cls Acc1: 41.949,                Cls Acc5: 76.483,                loss 2.149
Max acc1_cls: 41.95%
Epoch: [24]  [0/2]  eta: 0:00:08  lr: 0.000563  loss_total: 0.9962 (0.9962)  loss_cls: 0.9962 (0.9962)  acc1_c
ls: 88.2812 (88.2812)  acc5_cls: 92.1875 (92.1875)  time: 4.2521  data: 4.0347  max mem: 10232
Epoch: [24]  [1/2]  eta: 0:00:02  lr: 0.000563  loss_total: 0.9240 (0.9601)  loss_cls: 0.9240 (0.9601)  acc1_c
ls: 88.2812 (88.6719)  acc5_cls: 92.1875 (94.5312)  time: 2.2289  data: 2.0174  max mem: 10232
Epoch: [24] Total time: 0:00:04 (2.3676 s / it)
Averaged stats: lr: 0.000563  loss_total: 0.9240 (0.9446)  loss_cls: 0.9240 (0.9446)  acc1_cls: 88.2812 (89.11
13)  acc5_cls: 92.1875 (96.5820)
Test:  [0/4]  eta: 0:00:13  loss: 2.0938 (2.0938)  acc1_cls: 43.7500 (43.7500)  acc5_cls: 79.6875 (79.6875)  t
ime: 3.3507  data: 3.2962  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.0938 (2.1025)  acc1_cls: 39.0625 (43.2203)  acc5_cls: 79.6875 (77.3305)  t
ime: 0.8869  data: 0.8241  max mem: 10232
Test: Total time: 0:00:03 (0.9546 s / it)
* Batch-avg:             Cls Acc1: 43.220,                Cls Acc5: 77.331,                loss 2.102
Max acc1_cls: 43.22%
Epoch: [25]  [0/2]  eta: 0:00:08  lr: 0.000531  loss_total: 0.9624 (0.9624)  loss_cls: 0.9624 (0.9624)  acc1_c
ls: 90.6250 (90.6250)  acc5_cls: 94.5312 (94.5312)  time: 4.2943  data: 4.0915  max mem: 10232
Epoch: [25]  [1/2]  eta: 0:00:02  lr: 0.000531  loss_total: 0.9624 (0.9656)  loss_cls: 0.9624 (0.9656)  acc1_c
ls: 90.6250 (90.6250)  acc5_cls: 94.5312 (94.5312)  time: 2.2629  data: 2.0458  max mem: 10232
Epoch: [25] Total time: 0:00:04 (2.4073 s / it)
Averaged stats: lr: 0.000531  loss_total: 0.9624 (0.9095)  loss_cls: 0.9624 (0.9095)  acc1_cls: 90.6250 (90.77
15)  acc5_cls: 94.5312 (96.7285)
Test:  [0/4]  eta: 0:00:12  loss: 2.0573 (2.0573)  acc1_cls: 43.7500 (43.7500)  acc5_cls: 80.4688 (80.4688)  t
ime: 3.0967  data: 3.0419  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.0573 (2.1289)  acc1_cls: 35.9375 (41.9492)  acc5_cls: 79.5455 (78.3898)  t
ime: 0.8574  data: 0.8038  max mem: 10232
Test: Total time: 0:00:03 (0.9182 s / it)
* Batch-avg:             Cls Acc1: 41.949,                Cls Acc5: 78.390,                loss 2.129
Max acc1_cls: 43.22%
Epoch: [26]  [0/2]  eta: 0:00:10  lr: 0.000500  loss_total: 0.9528 (0.9528)  loss_cls: 0.9528 (0.9528)  acc1_c
ls: 88.2812 (88.2812)  acc5_cls: 94.5312 (94.5312)  time: 5.2037  data: 4.5984  max mem: 10232
Epoch: [26]  [1/2]  eta: 0:00:02  lr: 0.000500  loss_total: 0.8466 (0.8997)  loss_cls: 0.8466 (0.8997)  acc1_c
ls: 88.2812 (90.6250)  acc5_cls: 94.5312 (95.7031)  time: 2.7179  data: 2.2992  max mem: 10232
Epoch: [26] Total time: 0:00:05 (2.8819 s / it)
Averaged stats: lr: 0.000500  loss_total: 0.8466 (0.8663)  loss_cls: 0.8466 (0.8663)  acc1_cls: 88.2812 (90.72
27)  acc5_cls: 94.5312 (96.9238)
Test:  [0/4]  eta: 0:00:12  loss: 2.0459 (2.0459)  acc1_cls: 42.9688 (42.9688)  acc5_cls: 79.6875 (79.6875)  t
ime: 3.1734  data: 3.1189  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.0459 (2.1659)  acc1_cls: 31.2500 (39.1949)  acc5_cls: 76.1364 (78.8136)  t
ime: 0.8945  data: 0.8470  max mem: 10232
Test: Total time: 0:00:03 (0.9424 s / it)
* Batch-avg:             Cls Acc1: 39.195,                Cls Acc5: 78.814,                loss 2.166
Max acc1_cls: 43.22%
Epoch: [27]  [0/2]  eta: 0:00:10  lr: 0.000469  loss_total: 0.7615 (0.7615)  loss_cls: 0.7615 (0.7615)  acc1_c
ls: 92.9688 (92.9688)  acc5_cls: 97.6562 (97.6562)  time: 5.4000  data: 4.6940  max mem: 10232
Epoch: [27]  [1/2]  eta: 0:00:02  lr: 0.000469  loss_total: 0.7615 (0.8035)  loss_cls: 0.7615 (0.8035)  acc1_c
ls: 90.6250 (91.7969)  acc5_cls: 96.8750 (97.2656)  time: 2.8146  data: 2.3471  max mem: 10232
Epoch: [27] Total time: 0:00:05 (2.9624 s / it)
Averaged stats: lr: 0.000469  loss_total: 0.7615 (0.7752)  loss_cls: 0.7615 (0.7752)  acc1_cls: 90.6250 (92.91
99)  acc5_cls: 96.8750 (97.9980)
Test:  [0/4]  eta: 0:00:14  loss: 2.1223 (2.1223)  acc1_cls: 43.7500 (43.7500)  acc5_cls: 77.3438 (77.3438)  t
ime: 3.5364  data: 3.4813  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.1223 (2.2006)  acc1_cls: 33.5938 (39.4068)  acc5_cls: 72.7273 (76.9068)  t
ime: 0.9418  data: 0.8704  max mem: 10232
Test: Total time: 0:00:04 (1.0132 s / it)
* Batch-avg:             Cls Acc1: 39.407,                Cls Acc5: 76.907,                loss 2.201
Max acc1_cls: 43.22%
Epoch: [28]  [0/2]  eta: 0:00:10  lr: 0.000437  loss_total: 0.6759 (0.6759)  loss_cls: 0.6759 (0.6759)  acc1_c
ls: 96.0938 (96.0938)  acc5_cls: 99.2188 (99.2188)  time: 5.4229  data: 4.8559  max mem: 10232
Epoch: [28]  [1/2]  eta: 0:00:02  lr: 0.000437  loss_total: 0.6759 (0.7136)  loss_cls: 0.6759 (0.7136)  acc1_c
ls: 92.9688 (94.5312)  acc5_cls: 98.4375 (98.8281)  time: 2.8281  data: 2.4280  max mem: 10232
Epoch: [28] Total time: 0:00:05 (2.9939 s / it)
Averaged stats: lr: 0.000437  loss_total: 0.6759 (0.7531)  loss_cls: 0.6759 (0.7531)  acc1_cls: 92.9688 (92.67
58)  acc5_cls: 98.4375 (97.9004)
Test:  [0/4]  eta: 0:00:14  loss: 2.0274 (2.0274)  acc1_cls: 47.6562 (47.6562)  acc5_cls: 77.3438 (77.3438)  t
ime: 3.5947  data: 3.5412  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.0274 (2.0989)  acc1_cls: 39.8438 (43.2203)  acc5_cls: 74.2188 (78.1780)  t
ime: 0.9547  data: 0.8854  max mem: 10232
Test: Total time: 0:00:04 (1.0594 s / it)
* Batch-avg:             Cls Acc1: 43.220,                Cls Acc5: 78.178,                loss 2.099
Max acc1_cls: 43.22%
Epoch: [29]  [0/2]  eta: 0:00:08  lr: 0.000406  loss_total: 0.7825 (0.7825)  loss_cls: 0.7825 (0.7825)  acc1_c
ls: 89.8438 (89.8438)  acc5_cls: 97.6562 (97.6562)  time: 4.2293  data: 4.0269  max mem: 10232
Epoch: [29]  [1/2]  eta: 0:00:02  lr: 0.000406  loss_total: 0.7825 (0.7891)  loss_cls: 0.7825 (0.7891)  acc1_c
ls: 89.8438 (91.4062)  acc5_cls: 96.8750 (97.2656)  time: 2.2468  data: 2.0480  max mem: 10232
Epoch: [29] Total time: 0:00:04 (2.3933 s / it)
Averaged stats: lr: 0.000406  loss_total: 0.7825 (0.7283)  loss_cls: 0.7825 (0.7283)  acc1_cls: 89.8438 (93.79
88)  acc5_cls: 96.8750 (97.5586)
Test:  [0/4]  eta: 0:00:12  loss: 2.0180 (2.0180)  acc1_cls: 46.8750 (46.8750)  acc5_cls: 77.3438 (77.3438)  t
ime: 3.2192  data: 3.1655  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.0180 (2.0925)  acc1_cls: 41.4062 (44.4915)  acc5_cls: 74.2188 (77.3305)  t
ime: 0.9286  data: 0.8812  max mem: 10232
Test: Total time: 0:00:03 (0.9759 s / it)
* Batch-avg:             Cls Acc1: 44.492,                Cls Acc5: 77.331,                loss 2.093
Max acc1_cls: 44.49%
Epoch: [30]  [0/2]  eta: 0:00:08  lr: 0.000376  loss_total: 0.6686 (0.6686)  loss_cls: 0.6686 (0.6686)  acc1_c
ls: 96.0938 (96.0938)  acc5_cls: 100.0000 (100.0000)  time: 4.4391  data: 4.2479  max mem: 10232
Epoch: [30]  [1/2]  eta: 0:00:02  lr: 0.000376  loss_total: 0.6686 (0.7320)  loss_cls: 0.6686 (0.7320)  acc1_c
ls: 92.9688 (94.5312)  acc5_cls: 100.0000 (100.0000)  time: 2.3374  data: 2.1240  max mem: 10232
Epoch: [30] Total time: 0:00:04 (2.4766 s / it)
Averaged stats: lr: 0.000376  loss_total: 0.6686 (0.7214)  loss_cls: 0.6686 (0.7214)  acc1_cls: 92.9688 (93.94
53)  acc5_cls: 100.0000 (98.3887)
Test:  [0/4]  eta: 0:00:13  loss: 2.0071 (2.0071)  acc1_cls: 50.0000 (50.0000)  acc5_cls: 75.7812 (75.7812)  t
ime: 3.2922  data: 3.2363  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 2.0071 (2.0021)  acc1_cls: 47.6562 (47.2458)  acc5_cls: 75.7812 (77.7542)  t
ime: 0.8895  data: 0.8385  max mem: 10232
Test: Total time: 0:00:03 (0.9660 s / it)
* Batch-avg:             Cls Acc1: 47.246,                Cls Acc5: 77.754,                loss 2.002
Max acc1_cls: 47.25%
Epoch: [31]  [0/2]  eta: 0:00:08  lr: 0.000345  loss_total: 0.6194 (0.6194)  loss_cls: 0.6194 (0.6194)  acc1_c
ls: 92.9688 (92.9688)  acc5_cls: 97.6562 (97.6562)  time: 4.4325  data: 4.2387  max mem: 10232
Epoch: [31]  [1/2]  eta: 0:00:02  lr: 0.000345  loss_total: 0.5969 (0.6082)  loss_cls: 0.5969 (0.6082)  acc1_c
ls: 92.9688 (93.7500)  acc5_cls: 97.6562 (98.8281)  time: 2.3386  data: 2.1194  max mem: 10232
Epoch: [31] Total time: 0:00:05 (2.5039 s / it)
Averaged stats: lr: 0.000345  loss_total: 0.5969 (0.6577)  loss_cls: 0.5969 (0.6577)  acc1_cls: 92.9688 (94.04
30)  acc5_cls: 97.6562 (98.2910)
Test:  [0/4]  eta: 0:00:12  loss: 1.9119 (1.9119)  acc1_cls: 56.2500 (56.2500)  acc5_cls: 79.6875 (79.6875)  t
ime: 3.2116  data: 3.1556  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.9119 (1.9232)  acc1_cls: 52.3438 (51.6949)  acc5_cls: 78.9062 (80.7203)  t
ime: 0.9248  data: 0.8757  max mem: 10232
Test: Total time: 0:00:03 (0.9774 s / it)
* Batch-avg:             Cls Acc1: 51.695,                Cls Acc5: 80.720,                loss 1.923
Max acc1_cls: 51.69%
Epoch: [32]  [0/2]  eta: 0:00:08  lr: 0.000316  loss_total: 0.6363 (0.6363)  loss_cls: 0.6363 (0.6363)  acc1_c
ls: 93.7500 (93.7500)  acc5_cls: 100.0000 (100.0000)  time: 4.3475  data: 4.1536  max mem: 10232
Epoch: [32]  [1/2]  eta: 0:00:02  lr: 0.000316  loss_total: 0.6363 (0.6448)  loss_cls: 0.6363 (0.6448)  acc1_c
ls: 93.7500 (94.5312)  acc5_cls: 98.4375 (99.2188)  time: 2.2944  data: 2.0768  max mem: 10232
Epoch: [32] Total time: 0:00:04 (2.4449 s / it)
Averaged stats: lr: 0.000316  loss_total: 0.6363 (0.6350)  loss_cls: 0.6363 (0.6350)  acc1_cls: 93.7500 (94.92
19)  acc5_cls: 98.4375 (98.5352)
Test:  [0/4]  eta: 0:00:14  loss: 1.8149 (1.8149)  acc1_cls: 60.9375 (60.9375)  acc5_cls: 82.0312 (82.0312)  t
ime: 3.5536  data: 3.5000  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.8149 (1.8478)  acc1_cls: 58.5938 (56.7797)  acc5_cls: 79.5455 (81.3559)  t
ime: 0.9453  data: 0.8751  max mem: 10232
Test: Total time: 0:00:04 (1.0344 s / it)
* Batch-avg:             Cls Acc1: 56.780,                Cls Acc5: 81.356,                loss 1.848
Max acc1_cls: 56.78%
Epoch: [33]  [0/2]  eta: 0:00:09  lr: 0.000287  loss_total: 0.6263 (0.6263)  loss_cls: 0.6263 (0.6263)  acc1_c
ls: 93.7500 (93.7500)  acc5_cls: 97.6562 (97.6562)  time: 4.6706  data: 4.4753  max mem: 10232
Epoch: [33]  [1/2]  eta: 0:00:02  lr: 0.000287  loss_total: 0.6263 (0.6415)  loss_cls: 0.6263 (0.6415)  acc1_c
ls: 92.9688 (93.3594)  acc5_cls: 97.6562 (98.0469)  time: 2.4512  data: 2.2377  max mem: 10232
Epoch: [33] Total time: 0:00:05 (2.5945 s / it)
Averaged stats: lr: 0.000287  loss_total: 0.6263 (0.6205)  loss_cls: 0.6263 (0.6205)  acc1_cls: 92.9688 (94.58
01)  acc5_cls: 97.6562 (98.5840)
Test:  [0/4]  eta: 0:00:13  loss: 1.7657 (1.7657)  acc1_cls: 64.0625 (64.0625)  acc5_cls: 79.6875 (79.6875)  t
ime: 3.4309  data: 3.3778  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.7657 (1.7850)  acc1_cls: 61.7188 (60.5932)  acc5_cls: 79.6875 (81.5678)  t
ime: 0.9144  data: 0.8445  max mem: 10232
Test: Total time: 0:00:04 (1.0092 s / it)
* Batch-avg:             Cls Acc1: 60.593,                Cls Acc5: 81.568,                loss 1.785
Max acc1_cls: 60.59%
Epoch: [34]  [0/2]  eta: 0:00:08  lr: 0.000259  loss_total: 0.6379 (0.6379)  loss_cls: 0.6379 (0.6379)  acc1_c
ls: 93.7500 (93.7500)  acc5_cls: 98.4375 (98.4375)  time: 4.3823  data: 4.1756  max mem: 10232
Epoch: [34]  [1/2]  eta: 0:00:02  lr: 0.000259  loss_total: 0.5653 (0.6016)  loss_cls: 0.5653 (0.6016)  acc1_c
ls: 93.7500 (94.5312)  acc5_cls: 98.4375 (98.8281)  time: 2.3062  data: 2.0879  max mem: 10232
Epoch: [34] Total time: 0:00:04 (2.4557 s / it)
Averaged stats: lr: 0.000259  loss_total: 0.5653 (0.6056)  loss_cls: 0.5653 (0.6056)  acc1_cls: 93.7500 (95.21
48)  acc5_cls: 98.4375 (98.5352)
Test:  [0/4]  eta: 0:00:12  loss: 1.7221 (1.7221)  acc1_cls: 67.9688 (67.9688)  acc5_cls: 82.0312 (82.0312)  t
ime: 3.1463  data: 3.0913  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.7221 (1.7381)  acc1_cls: 63.2812 (63.3475)  acc5_cls: 82.0312 (82.8390)  t
ime: 0.8722  data: 0.8166  max mem: 10232
Test: Total time: 0:00:03 (0.9373 s / it)
* Batch-avg:             Cls Acc1: 63.347,                Cls Acc5: 82.839,                loss 1.738
Max acc1_cls: 63.35%
Epoch: [35]  [0/2]  eta: 0:00:08  lr: 0.000232  loss_total: 0.7197 (0.7197)  loss_cls: 0.7197 (0.7197)  acc1_c
ls: 92.1875 (92.1875)  acc5_cls: 95.3125 (95.3125)  time: 4.2244  data: 4.0310  max mem: 10232
Epoch: [35]  [1/2]  eta: 0:00:02  lr: 0.000232  loss_total: 0.6155 (0.6676)  loss_cls: 0.6155 (0.6676)  acc1_c
ls: 92.1875 (93.3594)  acc5_cls: 95.3125 (96.4844)  time: 2.2505  data: 2.0555  max mem: 10232
Epoch: [35] Total time: 0:00:04 (2.4008 s / it)
Averaged stats: lr: 0.000232  loss_total: 0.6155 (0.5709)  loss_cls: 0.6155 (0.5709)  acc1_cls: 92.1875 (95.89
84)  acc5_cls: 95.3125 (98.6328)
Test:  [0/4]  eta: 0:00:14  loss: 1.6707 (1.6707)  acc1_cls: 70.3125 (70.3125)  acc5_cls: 84.3750 (84.3750)  t
ime: 3.5182  data: 3.4636  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.6707 (1.6717)  acc1_cls: 64.8438 (65.8898)  acc5_cls: 84.3750 (84.3220)  t
ime: 0.9347  data: 0.8659  max mem: 10232
Test: Total time: 0:00:04 (1.0114 s / it)
* Batch-avg:             Cls Acc1: 65.890,                Cls Acc5: 84.322,                loss 1.672
Max acc1_cls: 65.89%
Epoch: [36]  [0/2]  eta: 0:00:09  lr: 0.000206  loss_total: 0.5039 (0.5039)  loss_cls: 0.5039 (0.5039)  acc1_c
ls: 96.0938 (96.0938)  acc5_cls: 98.4375 (98.4375)  time: 4.5641  data: 4.3765  max mem: 10232
Epoch: [36]  [1/2]  eta: 0:00:02  lr: 0.000206  loss_total: 0.5039 (0.5299)  loss_cls: 0.5039 (0.5299)  acc1_c
ls: 96.0938 (96.8750)  acc5_cls: 98.4375 (98.4375)  time: 2.3986  data: 2.1883  max mem: 10232
Epoch: [36] Total time: 0:00:05 (2.5555 s / it)
Averaged stats: lr: 0.000206  loss_total: 0.5039 (0.5496)  loss_cls: 0.5039 (0.5496)  acc1_cls: 96.0938 (96.38
67)  acc5_cls: 98.4375 (98.9746)
Test:  [0/4]  eta: 0:00:12  loss: 1.6467 (1.6467)  acc1_cls: 72.6562 (72.6562)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.2325  data: 3.1775  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.6467 (1.6311)  acc1_cls: 68.1818 (69.7034)  acc5_cls: 85.1562 (84.5339)  t
ime: 0.8931  data: 0.8450  max mem: 10232
Test: Total time: 0:00:03 (0.9633 s / it)
* Batch-avg:             Cls Acc1: 69.703,                Cls Acc5: 84.534,                loss 1.631
Max acc1_cls: 69.70%
Epoch: [37]  [0/2]  eta: 0:00:08  lr: 0.000181  loss_total: 0.6087 (0.6087)  loss_cls: 0.6087 (0.6087)  acc1_c
ls: 92.9688 (92.9688)  acc5_cls: 99.2188 (99.2188)  time: 4.3313  data: 4.1160  max mem: 10232
Epoch: [37]  [1/2]  eta: 0:00:02  lr: 0.000181  loss_total: 0.5739 (0.5913)  loss_cls: 0.5739 (0.5913)  acc1_c
ls: 92.9688 (93.3594)  acc5_cls: 99.2188 (99.2188)  time: 2.2760  data: 2.0580  max mem: 10232
Epoch: [37] Total time: 0:00:04 (2.4259 s / it)
Averaged stats: lr: 0.000181  loss_total: 0.5739 (0.5559)  loss_cls: 0.5739 (0.5559)  acc1_cls: 92.9688 (95.99
61)  acc5_cls: 99.2188 (99.1211)
Test:  [0/4]  eta: 0:00:13  loss: 1.6209 (1.6209)  acc1_cls: 71.8750 (71.8750)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.2610  data: 3.2073  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.6209 (1.5973)  acc1_cls: 71.8750 (72.2458)  acc5_cls: 85.1562 (85.1695)  t
ime: 0.8675  data: 0.8052  max mem: 10232
Test: Total time: 0:00:03 (0.9640 s / it)
* Batch-avg:             Cls Acc1: 72.246,                Cls Acc5: 85.169,                loss 1.597
Max acc1_cls: 72.25%
Epoch: [38]  [0/2]  eta: 0:00:08  lr: 0.000158  loss_total: 0.4963 (0.4963)  loss_cls: 0.4963 (0.4963)  acc1_c
ls: 96.8750 (96.8750)  acc5_cls: 98.4375 (98.4375)  time: 4.3934  data: 4.1811  max mem: 10232
Epoch: [38]  [1/2]  eta: 0:00:02  lr: 0.000158  loss_total: 0.4963 (0.5520)  loss_cls: 0.4963 (0.5520)  acc1_c
ls: 95.3125 (96.0938)  acc5_cls: 97.6562 (98.0469)  time: 2.3115  data: 2.0974  max mem: 10232
Epoch: [38] Total time: 0:00:04 (2.4775 s / it)
Averaged stats: lr: 0.000158  loss_total: 0.4963 (0.5195)  loss_cls: 0.4963 (0.5195)  acc1_cls: 95.3125 (96.28
91)  acc5_cls: 97.6562 (99.0234)
Test:  [0/4]  eta: 0:00:14  loss: 1.6008 (1.6008)  acc1_cls: 71.8750 (71.8750)  acc5_cls: 83.5938 (83.5938)  t
ime: 3.5054  data: 3.4507  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.6008 (1.5597)  acc1_cls: 71.8750 (73.7288)  acc5_cls: 83.5938 (84.1102)  t
ime: 0.9335  data: 0.8627  max mem: 10232
Test: Total time: 0:00:04 (1.0193 s / it)
* Batch-avg:             Cls Acc1: 73.729,                Cls Acc5: 84.110,                loss 1.560
Max acc1_cls: 73.73%
Epoch: [39]  [0/2]  eta: 0:00:08  lr: 0.000136  loss_total: 0.5276 (0.5276)  loss_cls: 0.5276 (0.5276)  acc1_c
ls: 96.8750 (96.8750)  acc5_cls: 100.0000 (100.0000)  time: 4.2749  data: 4.0624  max mem: 10232
Epoch: [39]  [1/2]  eta: 0:00:02  lr: 0.000136  loss_total: 0.5276 (0.5431)  loss_cls: 0.5276 (0.5431)  acc1_c
ls: 96.0938 (96.4844)  acc5_cls: 99.2188 (99.6094)  time: 2.2541  data: 2.0313  max mem: 10232
Epoch: [39] Total time: 0:00:04 (2.4090 s / it)
Averaged stats: lr: 0.000136  loss_total: 0.5276 (0.5135)  loss_cls: 0.5276 (0.5135)  acc1_cls: 96.0938 (96.09
38)  acc5_cls: 99.2188 (99.4141)
Test:  [0/4]  eta: 0:00:12  loss: 1.5850 (1.5850)  acc1_cls: 71.8750 (71.8750)  acc5_cls: 84.3750 (84.3750)  t
ime: 3.1555  data: 3.0997  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.5850 (1.5380)  acc1_cls: 71.8750 (74.3644)  acc5_cls: 84.3750 (84.3220)  t
ime: 0.9367  data: 0.8873  max mem: 10232
Test: Total time: 0:00:03 (0.9820 s / it)
* Batch-avg:             Cls Acc1: 74.364,                Cls Acc5: 84.322,                loss 1.538
Max acc1_cls: 74.36%
Epoch: [40]  [0/2]  eta: 0:00:08  lr: 0.000115  loss_total: 0.4737 (0.4737)  loss_cls: 0.4737 (0.4737)  acc1_c
ls: 97.6562 (97.6562)  acc5_cls: 99.2188 (99.2188)  time: 4.2594  data: 4.0654  max mem: 10232
Epoch: [40]  [1/2]  eta: 0:00:02  lr: 0.000115  loss_total: 0.4737 (0.5241)  loss_cls: 0.4737 (0.5241)  acc1_c
ls: 93.7500 (95.7031)  acc5_cls: 99.2188 (99.2188)  time: 2.3015  data: 2.1033  max mem: 10232
Epoch: [40] Total time: 0:00:04 (2.4649 s / it)
Averaged stats: lr: 0.000115  loss_total: 0.4737 (0.4965)  loss_cls: 0.4737 (0.4965)  acc1_cls: 93.7500 (96.58
20)  acc5_cls: 99.2188 (99.0234)
Test:  [0/4]  eta: 0:00:14  loss: 1.5512 (1.5512)  acc1_cls: 72.6562 (72.6562)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.6657  data: 3.6106  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.5512 (1.5184)  acc1_cls: 72.6562 (75.0000)  acc5_cls: 85.1562 (84.3220)  t
ime: 0.9718  data: 0.9027  max mem: 10232
Test: Total time: 0:00:04 (1.0818 s / it)
* Batch-avg:             Cls Acc1: 75.000,                Cls Acc5: 84.322,                loss 1.518
Max acc1_cls: 75.00%
Epoch: [41]  [0/2]  eta: 0:00:09  lr: 0.000096  loss_total: 0.4950 (0.4950)  loss_cls: 0.4950 (0.4950)  acc1_c
ls: 96.0938 (96.0938)  acc5_cls: 100.0000 (100.0000)  time: 4.9595  data: 4.7677  max mem: 10232
Epoch: [41]  [1/2]  eta: 0:00:02  lr: 0.000096  loss_total: 0.4495 (0.4722)  loss_cls: 0.4495 (0.4722)  acc1_c
ls: 95.3125 (95.7031)  acc5_cls: 99.2188 (99.6094)  time: 2.5982  data: 2.3839  max mem: 10232
Epoch: [41] Total time: 0:00:05 (2.7540 s / it)
Averaged stats: lr: 0.000096  loss_total: 0.4495 (0.4919)  loss_cls: 0.4495 (0.4919)  acc1_cls: 95.3125 (96.48
44)  acc5_cls: 99.2188 (99.4141)
Test:  [0/4]  eta: 0:00:13  loss: 1.5049 (1.5049)  acc1_cls: 73.4375 (73.4375)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.4178  data: 3.3624  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.5049 (1.5013)  acc1_cls: 72.7273 (74.3644)  acc5_cls: 85.1562 (84.5339)  t
ime: 0.9691  data: 0.9215  max mem: 10232
Test: Total time: 0:00:04 (1.0256 s / it)
* Batch-avg:             Cls Acc1: 74.364,                Cls Acc5: 84.534,                loss 1.501
Max acc1_cls: 75.00%
Epoch: [42]  [0/2]  eta: 0:00:11  lr: 0.000078  loss_total: 0.5377 (0.5377)  loss_cls: 0.5377 (0.5377)  acc1_c
ls: 96.8750 (96.8750)  acc5_cls: 99.2188 (99.2188)  time: 5.6035  data: 5.0326  max mem: 10232
Epoch: [42]  [1/2]  eta: 0:00:02  lr: 0.000078  loss_total: 0.4483 (0.4930)  loss_cls: 0.4483 (0.4930)  acc1_c
ls: 96.8750 (97.2656)  acc5_cls: 99.2188 (99.6094)  time: 2.9180  data: 2.5164  max mem: 10232
Epoch: [42] Total time: 0:00:06 (3.0384 s / it)
Averaged stats: lr: 0.000078  loss_total: 0.4483 (0.5185)  loss_cls: 0.4483 (0.5185)  acc1_cls: 96.8750 (96.48
44)  acc5_cls: 99.2188 (98.8281)
Test:  [0/4]  eta: 0:00:14  loss: 1.4808 (1.4808)  acc1_cls: 74.2188 (74.2188)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.5626  data: 3.5082  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.4808 (1.4870)  acc1_cls: 72.7273 (74.5763)  acc5_cls: 85.1562 (85.3814)  t
ime: 0.9472  data: 0.8771  max mem: 10232
Test: Total time: 0:00:04 (1.0506 s / it)
* Batch-avg:             Cls Acc1: 74.576,                Cls Acc5: 85.381,                loss 1.487
Max acc1_cls: 75.00%
Epoch: [43]  [0/2]  eta: 0:00:10  lr: 0.000062  loss_total: 0.4102 (0.4102)  loss_cls: 0.4102 (0.4102)  acc1_c
ls: 97.6562 (97.6562)  acc5_cls: 100.0000 (100.0000)  time: 5.3687  data: 4.6496  max mem: 10232
Epoch: [43]  [1/2]  eta: 0:00:02  lr: 0.000062  loss_total: 0.4102 (0.4543)  loss_cls: 0.4102 (0.4543)  acc1_c
ls: 97.6562 (97.6562)  acc5_cls: 99.2188 (99.6094)  time: 2.8402  data: 2.3248  max mem: 10232
Epoch: [43] Total time: 0:00:05 (2.9840 s / it)
Averaged stats: lr: 0.000062  loss_total: 0.4102 (0.4613)  loss_cls: 0.4102 (0.4613)  acc1_cls: 97.6562 (97.21
68)  acc5_cls: 99.2188 (99.4141)
Test:  [0/4]  eta: 0:00:13  loss: 1.4626 (1.4626)  acc1_cls: 74.2188 (74.2188)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.2610  data: 3.2061  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.4626 (1.4754)  acc1_cls: 72.7273 (74.5763)  acc5_cls: 85.1562 (85.5932)  t
ime: 0.9641  data: 0.9108  max mem: 10232
Test: Total time: 0:00:04 (1.0146 s / it)
* Batch-avg:             Cls Acc1: 74.576,                Cls Acc5: 85.593,                loss 1.475
Max acc1_cls: 75.00%
Epoch: [44]  [0/2]  eta: 0:00:11  lr: 0.000048  loss_total: 0.5296 (0.5296)  loss_cls: 0.5296 (0.5296)  acc1_c
ls: 96.0938 (96.0938)  acc5_cls: 99.2188 (99.2188)  time: 5.5165  data: 4.8454  max mem: 10232
Epoch: [44]  [1/2]  eta: 0:00:02  lr: 0.000048  loss_total: 0.4666 (0.4981)  loss_cls: 0.4666 (0.4981)  acc1_c
ls: 96.0938 (97.2656)  acc5_cls: 99.2188 (99.6094)  time: 2.8747  data: 2.4227  max mem: 10232
Epoch: [44] Total time: 0:00:06 (3.0235 s / it)
Averaged stats: lr: 0.000048  loss_total: 0.4666 (0.5068)  loss_cls: 0.4666 (0.5068)  acc1_cls: 96.0938 (96.04
49)  acc5_cls: 99.2188 (99.1699)
Test:  [0/4]  eta: 0:00:13  loss: 1.4388 (1.4388)  acc1_cls: 74.2188 (74.2188)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.4887  data: 3.4339  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.4388 (1.4591)  acc1_cls: 74.2188 (75.4237)  acc5_cls: 85.1562 (85.8051)  t
ime: 0.9600  data: 0.9111  max mem: 10232
Test: Total time: 0:00:04 (1.0260 s / it)
* Batch-avg:             Cls Acc1: 75.424,                Cls Acc5: 85.805,                loss 1.459
Max acc1_cls: 75.42%
Epoch: [45]  [0/2]  eta: 0:00:09  lr: 0.000035  loss_total: 0.5447 (0.5447)  loss_cls: 0.5447 (0.5447)  acc1_c
ls: 93.7500 (93.7500)  acc5_cls: 98.4375 (98.4375)  time: 4.6268  data: 4.4386  max mem: 10232
Epoch: [45]  [1/2]  eta: 0:00:02  lr: 0.000035  loss_total: 0.5223 (0.5335)  loss_cls: 0.5223 (0.5335)  acc1_c
ls: 93.7500 (94.5312)  acc5_cls: 98.4375 (98.8281)  time: 2.5389  data: 2.3438  max mem: 10232
Epoch: [45] Total time: 0:00:05 (2.6728 s / it)
Averaged stats: lr: 0.000035  loss_total: 0.5223 (0.4996)  loss_cls: 0.5223 (0.4996)  acc1_cls: 93.7500 (96.24
02)  acc5_cls: 98.4375 (99.0723)
Test:  [0/4]  eta: 0:00:13  loss: 1.4184 (1.4184)  acc1_cls: 75.7812 (75.7812)  acc5_cls: 84.3750 (84.3750)  t
ime: 3.4451  data: 3.3891  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.4184 (1.4445)  acc1_cls: 75.7812 (75.8475)  acc5_cls: 84.3750 (86.0169)  t
ime: 0.9179  data: 0.8473  max mem: 10232
Test: Total time: 0:00:04 (1.0273 s / it)
* Batch-avg:             Cls Acc1: 75.847,                Cls Acc5: 86.017,                loss 1.445
Max acc1_cls: 75.85%
Epoch: [46]  [0/2]  eta: 0:00:12  lr: 0.000024  loss_total: 0.5009 (0.5009)  loss_cls: 0.5009 (0.5009)  acc1_c
ls: 94.5312 (94.5312)  acc5_cls: 98.4375 (98.4375)  time: 6.4569  data: 6.2688  max mem: 10232
Epoch: [46]  [1/2]  eta: 0:00:03  lr: 0.000024  loss_total: 0.4863 (0.4936)  loss_cls: 0.4863 (0.4936)  acc1_c
ls: 94.5312 (95.3125)  acc5_cls: 98.4375 (98.8281)  time: 3.3449  data: 3.1345  max mem: 10232
Epoch: [46] Total time: 0:00:07 (3.5056 s / it)
Averaged stats: lr: 0.000024  loss_total: 0.4863 (0.4663)  loss_cls: 0.4863 (0.4663)  acc1_cls: 94.5312 (97.26
56)  acc5_cls: 98.4375 (99.5605)
Test:  [0/4]  eta: 0:00:14  loss: 1.4148 (1.4148)  acc1_cls: 75.7812 (75.7812)  acc5_cls: 84.3750 (84.3750)  t
ime: 3.5982  data: 3.5452  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.4148 (1.4317)  acc1_cls: 75.7812 (76.0593)  acc5_cls: 84.3750 (85.3814)  t
ime: 0.9551  data: 0.8863  max mem: 10232
Test: Total time: 0:00:04 (1.0476 s / it)
* Batch-avg:             Cls Acc1: 76.059,                Cls Acc5: 85.381,                loss 1.432
Max acc1_cls: 76.06%
Epoch: [47]  [0/2]  eta: 0:00:12  lr: 0.000016  loss_total: 0.4548 (0.4548)  loss_cls: 0.4548 (0.4548)  acc1_c
ls: 100.0000 (100.0000)  acc5_cls: 100.0000 (100.0000)  time: 6.1215  data: 5.9292  max mem: 10232
Epoch: [47]  [1/2]  eta: 0:00:03  lr: 0.000016  loss_total: 0.4308 (0.4428)  loss_cls: 0.4308 (0.4428)  acc1_c
ls: 97.6562 (98.8281)  acc5_cls: 100.0000 (100.0000)  time: 3.1778  data: 2.9647  max mem: 10232
Epoch: [47] Total time: 0:00:06 (3.3441 s / it)
Averaged stats: lr: 0.000016  loss_total: 0.4308 (0.4771)  loss_cls: 0.4308 (0.4771)  acc1_cls: 97.6562 (96.67
97)  acc5_cls: 100.0000 (99.6582)
Test:  [0/4]  eta: 0:00:13  loss: 1.4191 (1.4191)  acc1_cls: 75.7812 (75.7812)  acc5_cls: 84.3750 (84.3750)  t
ime: 3.4639  data: 3.4104  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.4191 (1.4200)  acc1_cls: 75.7812 (76.2712)  acc5_cls: 84.3750 (85.5932)  t
ime: 0.9219  data: 0.8526  max mem: 10232
Test: Total time: 0:00:04 (1.0181 s / it)
* Batch-avg:             Cls Acc1: 76.271,                Cls Acc5: 85.593,                loss 1.420
Max acc1_cls: 76.27%
Epoch: [48]  [0/2]  eta: 0:00:14  lr: 0.000009  loss_total: 0.4158 (0.4158)  loss_cls: 0.4158 (0.4158)  acc1_c
ls: 96.8750 (96.8750)  acc5_cls: 100.0000 (100.0000)  time: 7.1279  data: 6.9324  max mem: 10232
Epoch: [48]  [1/2]  eta: 0:00:03  lr: 0.000009  loss_total: 0.4158 (0.4612)  loss_cls: 0.4158 (0.4612)  acc1_c
ls: 96.0938 (96.4844)  acc5_cls: 100.0000 (100.0000)  time: 3.6815  data: 3.4662  max mem: 10232
Epoch: [48] Total time: 0:00:07 (3.8354 s / it)
Averaged stats: lr: 0.000009  loss_total: 0.4158 (0.4740)  loss_cls: 0.4158 (0.4740)  acc1_cls: 96.0938 (96.72
85)  acc5_cls: 100.0000 (99.4141)
Test:  [0/4]  eta: 0:00:12  loss: 1.4141 (1.4141)  acc1_cls: 75.7812 (75.7812)  acc5_cls: 84.3750 (84.3750)  t
ime: 3.2153  data: 3.1621  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.4141 (1.4062)  acc1_cls: 75.7812 (76.6949)  acc5_cls: 84.3750 (85.8051)  t
ime: 0.8583  data: 0.7991  max mem: 10232
Test: Total time: 0:00:03 (0.9433 s / it)
* Batch-avg:             Cls Acc1: 76.695,                Cls Acc5: 85.805,                loss 1.406
Max acc1_cls: 76.69%
Epoch: [49]  [0/2]  eta: 0:00:13  lr: 0.000004  loss_total: 0.4546 (0.4546)  loss_cls: 0.4546 (0.4546)  acc1_c
ls: 96.8750 (96.8750)  acc5_cls: 99.2188 (99.2188)  time: 6.9337  data: 6.7205  max mem: 10232
Epoch: [49]  [1/2]  eta: 0:00:03  lr: 0.000004  loss_total: 0.4546 (0.4710)  loss_cls: 0.4546 (0.4710)  acc1_c
ls: 94.5312 (95.7031)  acc5_cls: 97.6562 (98.4375)  time: 3.5772  data: 3.3661  max mem: 10232
Epoch: [49] Total time: 0:00:07 (3.7388 s / it)
Averaged stats: lr: 0.000004  loss_total: 0.4546 (0.4678)  loss_cls: 0.4546 (0.4678)  acc1_cls: 94.5312 (96.38
67)  acc5_cls: 97.6562 (99.0234)
Test:  [0/4]  eta: 0:00:14  loss: 1.3984 (1.3984)  acc1_cls: 75.7812 (75.7812)  acc5_cls: 85.1562 (85.1562)  t
ime: 3.5639  data: 3.5103  max mem: 10232
Test:  [3/4]  eta: 0:00:00  loss: 1.3984 (1.3966)  acc1_cls: 75.7812 (76.6949)  acc5_cls: 85.1562 (86.4407)  t
ime: 0.9466  data: 0.8776  max mem: 10232
Test: Total time: 0:00:04 (1.0518 s / it)
* Batch-avg:             Cls Acc1: 76.695,                Cls Acc5: 86.441,                loss 1.397
Max acc1_cls: 76.69%
Training time 0:24:59
wandb: Waiting for W&B process to finish... (success).
wandb: 🚀 View run in1k_jigsaw_small_patch56_336_e30_c1000frcl50 at: https://wandb.ai/doem97/Puzzle/runs/6wife
ril
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20231102_185907-6wiferil/logs
./exps/dgx/jigsaw_small_p56_336_in1k_c1000frcl50.sh: line 32: --log_dir: command not found
(study) root@e20a9f9e5a00:/workspace/study/permvit# tmux list-panes -a -F "#{session_name}:#{window_index}.#{p
ane_index} #{pane_current_command} #{pane_current_path}"
0:0.0 bash /workspace/study/temp/JigsawViT/imagenet/jigsaw-deit
0:1.0 bash /workspace/study
0:1.1 bash /workspace/study/efm
0:2.0 watch /workspace/data/study/efm/outputs/jigsaw_in1k_336_56 (deleted)
0:2.1 tmux /workspace/study/permvit
0:4.0 bash /workspace/study/temp/final/imagenet/jigsaw-deit
(study) root@e20a9f9e5a00:/workspace/study/permvit# la
.git         datasets.py        logs              outputs           permsets.py       utils.py
.gitignore   engine.py          losses.py         perm6x6x1000.npy  preds             wandb
.vscode      engine_jigsaw.py   main.py           perm6x6x3.npy     requirements.txt
__pycache__  exps               main_jigsaw.py    perm6x6x50.npy    samplers.py
augment.py   inference_perm.py  models.py         perm6x6x500.npy   scripts
data         log.txt            models_jigsaw.py  perm6x6x50x6.npy  tox.ini
(study) root@e20a9f9e5a00:/workspace/study/permvit# tmux capture-pane -S - -E - -t 0:0.0 -p > output.txt
(study) root@e20a9f9e5a00:/workspace/study/permvit# tmux capture-pane -S - -E - -t 0:2.1 -p > output.txt

