batch_size: 128
epochs: 100
bce_loss: True
unscale_lr: True
rec: False
freeze: True
model: jigsaw_small_patch56_336
input_size: 336
permcls: 1000
drop: 0.0
drop_path: 0.1
model_ema: True
model_ema_decay: 0.99996
model_ema_force_cpu: False
opt: adamw
opt_eps: 1e-08
opt_betas: None
clip_grad: None
momentum: 0.9
weight_decay: 0.05
sched: cosine
lr: 0.001
lr_noise: None
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1e-06
min_lr: 1e-07
decay_epochs: 30
warmup_epochs: 0
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
color_jitter: 0.3
aa: rand-m9-mstd0.5-inc1
smoothing: None
train_interpolation: bicubic
repeated_aug: True
train_mode: True
ThreeAugment: False
src: False
reprob: 0.25
remode: pixel
recount: 1
resplit: False
mixup: 0.0
cutmix: 0.0
cutmix_minmax: None
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
teacher_model: regnety_160
teacher_path: 
distillation_type: none
distillation_alpha: 0.5
distillation_tau: 1.0
finetune: ./outputs/in1k_jigsaw_small_patch56_336_e30_c1000/best_checkpoint.pth
attn_only: False
data_path: /workspace/data/imagenet/ILSVRC/Data/CLS-LOC
data_set: IMNET
nb_classes: 50
inat_category: name
output_dir: ./outputs/jigsaw_small_p56_336_in1k_c1000frcl50_bs512e100
log_dir: ./logs/jigsaw_small_p56_336_in1k_c1000frcl50_bs512e100
device: cuda
seed: 0
resume: 
start_epoch: 0
eval: False
eval_crop_ratio: 0.875
dist_eval: False
num_workers: 10
pin_mem: True
world_size: 4
dist_url: env://
local_rank: 0
use_jigsaw: True
use_cls: True
lambda_rec: 0.1
mask_ratio: 0.0
rank: 0
gpu: 0
distributed: True
dist_backend: nccl

Evaluation on epoch 0: loss: 22.259, acc1_cls: 1.907, acc5_cls: 10.593
{"train_lr": 0.001, "train_loss_total": 4.215037286281586, "train_loss_cls": 4.215037286281586, "train_acc1_cls": 6.005859375, "train_acc5_cls": 19.140625, "epoch": 0, "n_parameters": 319077484}
Evaluation on epoch 1: loss: 17.269, acc1_cls: 1.907, acc5_cls: 11.653
{"train_lr": 0.001, "train_loss_total": 4.195353984832764, "train_loss_cls": 4.195353984832764, "train_acc1_cls": 6.982421875, "train_acc5_cls": 22.65625, "epoch": 1, "n_parameters": 319077484}
Evaluation on epoch 2: loss: 10.283, acc1_cls: 1.907, acc5_cls: 11.653
{"train_lr": 0.0009997533048548475, "train_loss_total": 3.96928334236145, "train_loss_cls": 3.96928334236145, "train_acc1_cls": 8.49609375, "train_acc5_cls": 25.87890625, "epoch": 2, "n_parameters": 319077484}
Evaluation on epoch 3: loss: 6.001, acc1_cls: 2.119, acc5_cls: 11.017
{"train_lr": 0.0009990134628777144, "train_loss_total": 3.953603744506836, "train_loss_cls": 3.953603744506836, "train_acc1_cls": 7.32421875, "train_acc5_cls": 26.025390625, "epoch": 3, "n_parameters": 319077484}
Evaluation on epoch 4: loss: 7.982, acc1_cls: 2.119, acc5_cls: 10.593
{"train_lr": 0.00099778120420331, "train_loss_total": 3.887292265892029, "train_loss_cls": 3.887292265892029, "train_acc1_cls": 10.64453125, "train_acc5_cls": 30.859375, "epoch": 4, "n_parameters": 319077484}
Evaluation on epoch 5: loss: 6.959, acc1_cls: 2.754, acc5_cls: 10.381
{"train_lr": 0.0009960577449221733, "train_loss_total": 3.7812949419021606, "train_loss_cls": 3.7812949419021606, "train_acc1_cls": 15.234375, "train_acc5_cls": 32.51953125, "epoch": 5, "n_parameters": 319077484}
Evaluation on epoch 6: loss: 8.375, acc1_cls: 2.119, acc5_cls: 16.102
{"train_lr": 0.000993844785880539, "train_loss_total": 3.6715537309646606, "train_loss_cls": 3.6715537309646606, "train_acc1_cls": 17.919921875, "train_acc5_cls": 35.05859375, "epoch": 6, "n_parameters": 319077484}
Evaluation on epoch 7: loss: 6.469, acc1_cls: 5.297, acc5_cls: 14.195
{"train_lr": 0.000991144511001808, "train_loss_total": 3.441774845123291, "train_loss_cls": 3.441774845123291, "train_acc1_cls": 23.193359375, "train_acc5_cls": 42.87109375, "epoch": 7, "n_parameters": 319077484}
Evaluation on epoch 8: loss: 8.271, acc1_cls: 2.542, acc5_cls: 17.161
{"train_lr": 0.0009879595851312768, "train_loss_total": 3.292898416519165, "train_loss_cls": 3.292898416519165, "train_acc1_cls": 26.7578125, "train_acc5_cls": 45.947265625, "epoch": 8, "n_parameters": 319077484}
Evaluation on epoch 9: loss: 7.171, acc1_cls: 3.602, acc5_cls: 12.288
{"train_lr": 0.000984293151406259, "train_loss_total": 3.1018588542938232, "train_loss_cls": 3.1018588542938232, "train_acc1_cls": 31.787109375, "train_acc5_cls": 53.02734375, "epoch": 9, "n_parameters": 319077484}
